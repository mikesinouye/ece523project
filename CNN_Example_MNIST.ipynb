{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNISTBEEE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlwoMAiuZBpl"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# General Imports:\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "\n",
        "# From Imports:\n",
        "from tensorflow.keras import layers, activations, initializers, regularizers, constraints, Model\n",
        "\n",
        "@tf.custom_gradient\n",
        "def custom_round(x):\n",
        "\toutput = tf.keras.backend.round(x)\n",
        "\tdef grad(dy):\n",
        "\t\treturn dy#*(tf.keras.backend.maximum(0*x, 1-2*tf.keras.backend.abs(x-0.5)) + tf.keras.backend.maximum(0*x, 1-2*tf.keras.backend.abs(x+0.5)))\n",
        "\treturn output, grad\n",
        "\n",
        "class CWTConv2D(layers.Layer) :\n",
        "\t\n",
        "\tdef __init__(self, filters, kernel_size, strides, use_bias=True, **kwargs):\n",
        "\t\tsuper(CWTConv2D, self).__init__(**kwargs)\n",
        "\t\t\n",
        "\t\tself.filters = filters\n",
        "\t\tself.kernel_size = kernel_size\n",
        "\t\tself.strides = strides\n",
        "\t\tself.use_bias = use_bias\n",
        "\n",
        "\tdef build(self, input_shape):\n",
        "\t\t\n",
        "\t\tself.kernel = self.add_weight(name='kernel',\n",
        "\t\t\t\t\t\t\t\t\t\t   shape=(self.kernel_size[0], self.kernel_size[1], input_shape[-1], self.filters),\n",
        "\t\t\t\t\t\t\t\t\t\t   initializer=initializers.RandomNormal(mean=0.0, stddev=0.5, seed=0),\n",
        "\t\t\t\t\t\t\t\t\t\t   trainable=True)\n",
        "\n",
        "\t\tif (self.use_bias) :\t\t\t\t\t\t\t   \n",
        "\t\t\tself.biases = self.add_weight(name='biases',\n",
        "\t\t\t\t\t\t\t\t\t\t\t  shape=(self.filters),\n",
        "\t\t\t\t\t\t\t\t\t\t\t  initializer='zeros',\n",
        "\t\t\t\t\t\t\t\t\t\t\t  trainable=True)\n",
        "\t\t\t\t\t\t\t\t\t\t  \n",
        "\t\tsuper(CWTConv2D, self).build(input_shape)\n",
        "\t\t\t\t\t\t\t\t\t\t  \n",
        "\tdef call(self, inputs):\n",
        "\t\t\n",
        "\t\t#inputs = tf.keras.backend.in_train_phase(inputs, tf.keras.backend.round(inputs))\n",
        "\t\t\n",
        "\t\tkernel = self.kernel\n",
        "\t\t\n",
        "\t\t#kernel = tf.keras.backend.round(kernel)\n",
        "\t\tkernel = custom_round(kernel)\n",
        "\t\t\n",
        "\t\tkernel = tf.keras.backend.clip(kernel, -1, 1)\n",
        "\t\t\n",
        "\t\toutput = tf.keras.backend.conv2d(inputs, kernel=kernel, strides=self.strides, padding='valid', data_format=\"channels_last\")\n",
        "\t\t\n",
        "\t\tif (self.use_bias) :\n",
        "\t\t\tbiases = self.biases\n",
        "\t\t\t\n",
        "\t\t\t#biases = tf.keras.backend.round(biases)\n",
        "\t\t\tbiases = custom_round(biases)\n",
        "\t\t\t\n",
        "\t\t\tbiases = tf.keras.backend.clip(biases, -128, 127)\n",
        "\t\t\t\n",
        "\t\t\toutput = tf.keras.backend.bias_add(output, biases, data_format='channels_last')\n",
        "\t\t\n",
        "\t\toutput = tf.keras.backend.in_train_phase(activations.sigmoid(output), tf.dtypes.cast(tf.math.greater_equal(output, 0.0), tf.float32))\n",
        "\t\t\n",
        "\t\t#output = tf.keras.backend.clip(output, 0, 1)\n",
        "\t\t\n",
        "\t\t#output = tf.keras.backend.round(output)\n",
        "\t\t#output = custom_round(output)\n",
        "\t\t\n",
        "\t\treturn output\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t \n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\t\n",
        "\t\treturn ((inputs.shape[0] - self.kernel_size[0])/self.strides[0] + 1, (inputs.shape[1] - self.kernel_size[1])/self.strides[1] + 1, self.filters)\n",
        "\t\n",
        "\n",
        "# custom cwt constrain function\n",
        "def constrain_weights_cwt(model) :\n",
        "\n",
        "\t# Slam the convolutional kernel weights to -1, 0, or 1\n",
        "\tweights = model.get_weights()\n",
        "\torig_weights = copy.deepcopy(weights)\n",
        "\tnames = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "\t\n",
        "\tlayer_num = -1\n",
        "\tfor weight, name in zip(weights, names):\n",
        "\t\tlayer_num += 1\n",
        "\n",
        "\t\tif 'cwt' in name and 'kernel' in name:\n",
        "\t\t\tweight = tf.keras.backend.round(weight)\n",
        "\t\t\tweight = tf.keras.backend.clip(weight, -1, 1)\n",
        "\t\t\tweights[layer_num] = weight\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\tif 'cwt' in name and 'bias' in name:\t\n",
        "\t\t\tweight = tf.keras.backend.round(weight)\n",
        "\t\t\tweight = tf.keras.backend.clip(weight, -128, 127)\n",
        "\t\t\tweights[layer_num] = weight\n",
        "\t\t\t\n",
        "\treturn orig_weights, weights\n",
        "\t\n",
        "# rounds a Keras Conv2D layer's weights to a ternary value (-1, 0, 1) to \n",
        "# be compatible with deployment onto a SNN based simulator/enviornment\n",
        "# finds the average weight, and then uses the mean as a reference point for where it should go\n",
        "# this should be deprecated once the train while constrain approach is implemented\n",
        "def constrain_weights(model) :\n",
        "\n",
        "\t# Slam the convolutional kernel weights to -1, 0, or 1\n",
        "\tweights = model.get_weights()\n",
        "\torig_weights = copy.deepcopy(weights)\n",
        "\tnames = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "\t\n",
        "\tlayer_num = -1\n",
        "\tfor weight, name in zip(weights, names):\n",
        "\t\tlayer_num += 1\n",
        "\n",
        "\t\tif 'kernel' in name:\t\n",
        "\t\t\told_shape = weight.shape\n",
        "\t\t\tflattened_weights = weight.flatten()\n",
        "\t\t\tabsolute_weights = np.absolute(flattened_weights)\n",
        "\t\t\tmean = np.mean(absolute_weights)\n",
        "\t\t\tfor i in range(len(flattened_weights)) :\n",
        "\t\t\t\tif (flattened_weights[i] > mean) :\n",
        "\t\t\t\t\tflattened_weights[i] = 1\n",
        "\t\t\t\telif (flattened_weights[i] < -1 * mean) :\n",
        "\t\t\t\t\tflattened_weights[i] = -1\n",
        "\t\t\t\telse :\n",
        "\t\t\t\t\tflattened_weights[i] = 0\n",
        "\t\t\tweight = flattened_weights.reshape(old_shape)\n",
        "\t\t\tweights[layer_num] = weight\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\tif 'bias' in name:\t\n",
        "\t\t\told_shape = weight.shape\n",
        "\t\t\tflattened_weights = weight.flatten()\n",
        "\t\t\tabsolute_weights = np.absolute(flattened_weights)\n",
        "\t\t\tmean = np.mean(absolute_weights)\n",
        "\t\t\tfor i in range(len(flattened_weights)) :\n",
        "\t\t\t\tif (flattened_weights[i] > mean) :\n",
        "\t\t\t\t\tflattened_weights[i] = 1\n",
        "\t\t\t\telif (flattened_weights[i] < -1 * mean) :\n",
        "\t\t\t\t\tflattened_weights[i] = -1\n",
        "\t\t\t\telse :\n",
        "\t\t\t\t\tflattened_weights[i] = 0\n",
        "\t\t\tweight = flattened_weights.reshape(old_shape)\n",
        "\t\t\tweights[layer_num] = weight\n",
        "\t\t\t\n",
        "\treturn orig_weights, weights\t\n",
        "\t\n",
        "def print_weights(model) :\n",
        "\tview_weights = model.get_weights()\n",
        "\tnames = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "\tfor weight, name in zip(view_weights, names) :\n",
        "\t\tif 'kernel' in name:\t\n",
        "\t\t\tprint(name + \" weights: \")\n",
        "\t\t\tprint(weight.shape)\n",
        "\t\t\tprint_weights = weight\n",
        "\t\t\t\n",
        "\t\n",
        "\tfor f in range(print_weights.shape[3]) :\n",
        "\t\tfor y in range(print_weights.shape[0]) :\n",
        "\t\t\tline = \"\"\n",
        "\t\t\tfor x in range(print_weights.shape[1]) :\n",
        "\t\t\t\tfor c in range (print_weights.shape[2]) :\n",
        "\t\t\t\t\tline += str(print_weights[y][x][c][f])\n",
        "\t\t\t\tline += \" \"\n",
        "\t\t\tprint(line)\n",
        "\t\tprint(\"Next Filter: \")\n",
        "\t\t\n",
        "\tfor weight, name in zip(view_weights, names) :\n",
        "\t\tif 'bias' in name:\t\n",
        "\t\t\tprint(name + \" weights : \")\n",
        "\t\t\tprint(weight)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGWb0MlJ3yTU"
      },
      "source": [
        "# Contains the Code for Tea Layer for use in TeaLarning and TensorFlow 2.0\n",
        "\n",
        "\n",
        "# Future Imports:\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# General Imports:\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# From Imports:\n",
        "from tensorflow.keras import layers, activations, initializers, regularizers, constraints\n",
        "#from tensorflow.keras import backend as K\n",
        "# Custom Round Gradient:\n",
        "# In a TeaLayer the connections are rounded during the feedforward process.\n",
        "# We need a custom rounding function to implement this.\n",
        "# For this function the gradient is treated as a [ 1 ] so as not to effect backprop.\n",
        "\n",
        "@tf.custom_gradient\n",
        "def CustomRound(x):\n",
        "\toutput = tf.keras.backend.round(x)\n",
        "\tdef grad(dy):\n",
        "\t\treturn dy\n",
        "\treturn output, grad\n",
        "######################################################################################################################################\n",
        "class Tea(layers.Layer):\n",
        "\t\"\"\"\n",
        "\tThe following is an implementation of a TeaLayer used to implement IBM's TeaLarning Training\n",
        "\t\tmethod for RANC-based TrueNorth deployment.\n",
        "\tFor this to be compatable with the TrueNorth architecture, some irregular constraints and \n",
        "\t\tfunctionalities must be implemented.\n",
        "\tEach layer contains stastically initialized [ weights ], which are multiplied by trainable\n",
        "\t\t[ connections ]. [ Connections ] are floating point values which represent the\n",
        "\t\tprobablity that a [ connection ] exists. When feeding-forward [ connections ] are normally\n",
        "\t\tconstrained to the values of 1, if >= 0.5, and 0, else. \n",
        "\t\tThis method allows for feed-forward to represent actual TrueNorth computations, but still\n",
        "\t\tallows connections to be trained during backprop.\n",
        "\tAdditionally, inputs into the TrueNorth layer must be constrained to binary spikes of 1 or 0.\n",
        "\t\tInput data is normalized between 0 and 1, then if the value is 0.5 or greater it is represented\n",
        "\t\tas a spike (1), otherwise it is represented as a non-spike (0).\n",
        "\tFinally, outputs of the layer must be constrained during validation and testing. After weighted\n",
        "\t\tinputs are calculated, each value is set to 1 if it is greater than or equal to 0, and 0 otherwise.\n",
        "\t\tDuring training, this is estimated by a sigmoid activation function.\n",
        "\t\"\"\"\n",
        "\n",
        "\t\"\"\"\n",
        "\tNew TeaLayer Initialization:\n",
        "\tArguments:\n",
        "\t\tunits -- The number of neurons to use for this layer.\n",
        "\tKeyword Arguments:\n",
        "\t\tactivation -- The type of activation function to use to estimate spiking during training.\n",
        "\t\t\t\t\t  Note: Sigmoid activation function is specifically chosen to most accurately\n",
        "\t\t\t\t\t\trepresent spiking. This value [should] be left as the default.\n",
        "\t\t\t\t\t  DEFAULT: [sigmoid]\n",
        "\t\tuse_bias   -- To use biases or not.\n",
        "\t\t\t\t\t  DEFAULT: [True]\n",
        "\t\tweight_initializer -- The initializer to use when initializing the weights. If [None], then\n",
        "\t\t\t\t\t\t\t  the function [tea_weight_initializer] is used. This function sets all\n",
        "\t\t\t\t\t\t\t  weights to be -1 or 1 as outlined by IBM's TeaLarning Literature.\n",
        "\t\t\t\t\t\t\t  DEFAULT: [None]\n",
        "\t\tbias_initalizer -- The initializer to use when initializing biases.\n",
        "\t\t\t\t\t\t   DEFAULT: [zeros]\n",
        "\t\tconnection_initializer -- The initializer to use when initializing connection values.\n",
        "\t\t\t\t\t\t\t\t  Note: Connections should be initialized as a [probability distribution].\n",
        "\t\t\t\t\t\t\t\t  By default they are sampled from a normal distrubtion with a mean of 0.5\n",
        "\t\t\t\t\t\t\t\t  DEFAULT: [None]\n",
        "\t\tconnection_regularizer -- A regularizer to use on the connection values, if any.\n",
        "\t\t\t\t\t\t\t\t  DEFAULT: [None]\n",
        "\t\t\n",
        "\t\tconnection_constraint -- A constraint to apply to the connections, if any.\n",
        "\t\t\t\t\t\t\t\t DEFAULT: [None]\n",
        "\t\tround_input -- When feeding-forward this option dictates to round the input values or not to. This should\n",
        "\t\t\t\t\t   be set to [True] to maintain spiking simulation.\n",
        "\t\t\t\t\t   DEFAULT: [True]\n",
        "\t\t\n",
        "\t\tround_connections -- When feeding-forward this option dictates to round the connection values or not.\n",
        "\t\t\t\t\t\t\t DEFAULT: [True]\n",
        "\t\tclip_connection -- This option dictates if connection values should be clipped between 0 and 1 when feeding-forward.\n",
        "\t\t\t\t\t\t   DEFAULT: [True]\n",
        "\t\tround_bias -- This option dictates if biases should be rounded when feeding-forward.\n",
        "\t\t\t\t\t  DEFAULT: [True]\n",
        "\t\tconstrain_after_train -- This option dictates if outputs should be constrained to spikes (0 or 1) when training is completed.\n",
        "\t\t\t\t\t\t\t\t DEFAULT: [True]\n",
        "\t\"\"\"\n",
        "\t##################################################################################################################################\n",
        "\tdef __init__(self,\n",
        "\t\t\t\t units,\n",
        "\t\t\t\t activation='sigmoid',\n",
        "\t\t\t\t use_bias=True,\n",
        "\t\t\t\t weight_initializer=None,\n",
        "\t\t\t\t bias_initializer='zeros',\n",
        "\t\t\t\t connection_initializer=None,\n",
        "\t\t\t\t connection_regularizer=None,\n",
        "\t\t\t\t connection_constraint=None,\n",
        "\t\t\t\t round_input=True,\n",
        "\t\t\t\t round_connections=True,\n",
        "\t\t\t\t clip_connections=True,\n",
        "\t\t\t\t round_bias=True,\n",
        "\t\t\t\t constrain_after_train=True,\n",
        "\t\t\t\t **kwargs):\n",
        "\t\tsuper(Tea, self).__init__(**kwargs)\n",
        "\t\t\n",
        "\t\tself.units=units\n",
        "\t\t\n",
        "\t\tself.activation=activations.get(activation)\n",
        "\t\t\n",
        "\t\tself.use_bias=use_bias\n",
        "\n",
        "\t\tif connection_initializer:\n",
        "\t\t\tself.connection_initializer=connection_initializer\n",
        "\t\telse:\n",
        "\t\t\tself.connection_initializer=initializers.TruncatedNormal(mean=0.5, seed=0)\n",
        "\n",
        "\t\tif weight_initializer:\n",
        "\t\t\tself.weight_initializer=weight_initializer\n",
        "\t\telse:\n",
        "\t\t\tself.weight_initializer=tea_weight_initializer\n",
        "\n",
        "\t\tself.bias_initializer=bias_initializer\n",
        "\n",
        "\t\tself.connection_regularizer=connection_regularizer\n",
        "\t\t\n",
        "\t\tself.connection_constraint=connection_constraint\n",
        "\n",
        "\t\tself.input_width=None\n",
        "\n",
        "\t\tself.round_input=round_input\n",
        "\n",
        "\t\tself.round_connections=round_connections\n",
        "\t\t\n",
        "\t\tself.clip_connections=clip_connections\n",
        "\n",
        "\t\tself.round_bias=round_bias\n",
        "\n",
        "\t\tself.constrain_after_train=constrain_after_train\n",
        "\n",
        "\t\tself.uses_learning_phase=True\n",
        "\t##################################################################################################################################\n",
        "\tdef build(self, input_shape):\n",
        "\t\tassert len(input_shape) >= 2\n",
        "\n",
        "\t\tsuper(Tea,self).build(input_shape)\n",
        "\n",
        "\t\tshape = (input_shape[-1], self.units)\n",
        "\n",
        "\t\tself.static_weights = self.add_weight(name='weights',\n",
        "\t\t\t\t\t\t\t\t\t\t\t  shape=shape,\n",
        "\t\t\t\t\t\t\t\t\t\t\t  initializer=self.weight_initializer,\n",
        "\t\t\t\t\t\t\t\t\t\t\t  trainable=False)\n",
        "\n",
        "\t\tself.connections = self.add_weight(name='connections',\n",
        "\t\t\t\t\t\t\t\t\t\t   shape=shape,\n",
        "\t\t\t\t\t\t\t\t\t\t   initializer=self.connection_initializer,\n",
        "\t\t\t\t\t\t\t\t\t\t   regularizer=self.connection_regularizer,\n",
        "\t\t\t\t\t\t\t\t\t\t   constraint=self.connection_constraint)\n",
        "\n",
        "\t\tif self.use_bias:\n",
        "\t\t\tself.biases = self.add_weight(name='bias',\n",
        "\t\t\t\t\t\t\t\t\t\t  shape=(self.units,),\n",
        "\t\t\t\t\t\t\t\t\t\t  initializer=self.bias_initializer)\n",
        "\t##################################################################################################################################\n",
        "\tdef call(self, inputs):\n",
        "\n",
        "\t\t# Constrain the Input:\n",
        "\t\tif self.round_input:\n",
        "\t\t\tinputs = CustomRound(inputs)\n",
        "\t\telse:\n",
        "\t\t\tinputs = tf.keras.backend.in_train_phase(inputs, CustomRound(inputs))\n",
        "\n",
        "\t\t# Connection Constraints:\n",
        "\t\tconnections = self.connections\n",
        "\n",
        "\t\tif self.round_connections:\n",
        "\t\t\tconnections = CustomRound(connections)\n",
        "\t\telse:\n",
        "\t\t\tconnections = tf.keras.backend.in_train_phase(connections, CustomRound(connections))\n",
        "\n",
        "\t\tif self.clip_connections:\n",
        "\t\t\tconnections = tf.keras.backend.clip(connections, 0, 1)\n",
        "\t\telse:\n",
        "\t\t\tconnections = tf.keras.backend.in_train_phase(connections, tf.keras.backend.clip(connections, 0, 1))\n",
        "\n",
        "\t\t# Multiply Connections with Weights:\n",
        "\t\tweighted_connections = connections * self.static_weights\n",
        "\n",
        "\t\t# Dot Product the Input with the Weighted Connections\n",
        "\t\toutput = tf.keras.backend.dot(inputs, weighted_connections)\n",
        "\n",
        "\t\t# Add biases if they are being used:\n",
        "\t\tif self.use_bias:\n",
        "\t\t\t\n",
        "\t\t\t# Constrain the biases first:\n",
        "\t\t\tif self.round_bias:\n",
        "\t\t\t\tbiases = CustomRound(self.biases)\n",
        "\t\t\telse:\n",
        "\t\t\t\tbiases = tf.keras.backend.in_train_phase(self.biases, CustomRound(self.biases))\n",
        "\n",
        "\t\t\toutput = tf.keras.backend.bias_add(output, biases, data_format='channels_last')\n",
        "\n",
        "\t\t# Apply activation / Spike(s)\n",
        "\t\toutput = tf.keras.backend.in_train_phase(self.activation(output), \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t tf.dtypes.cast(tf.math.greater_equal(output, 0.0), tf.float32))\n",
        "\n",
        "\t\treturn output\n",
        "\t##################################################################################################################################\n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\tassert input_shape and len(input_shape) >= 2\n",
        "\t\tassert input_shape[-1]\n",
        "\n",
        "\t\toutput_shape = list(input_shape)\n",
        "\n",
        "\t\toutput_shape[-1] = self.units\n",
        "\n",
        "\t\treturn tuple(output_shape)\n",
        "\t##################################################################################################################################\n",
        "# END CLASS : TEA\n",
        "\"\"\"\n",
        "This function returns a tensor of alternating 1s and -1s. This is a basic re-implementation of IBM's own weight matrix initializations.\n",
        "Argument:\n",
        "\tshape -- The shape of the weights to be initialized.\n",
        "Keyword Arguments:\n",
        "\tdtype -- The data type to be used when initializing the weights.\n",
        "\t\t\t DEFAULT : [np.float32]\n",
        "\"\"\"\n",
        "def tea_weight_initializer(shape, dtype=np.float32):\n",
        "\tnum_axons = shape[0]\n",
        "\tnum_neurons = shape[1]\n",
        "\tif dtype == 'float32':\n",
        "\t\tdtype = np.float32\n",
        "\tret_array = np.zeros((int(num_axons), int(num_neurons)), dtype=dtype)\n",
        "\n",
        "\tfor axon_num, axon in enumerate(ret_array):\n",
        "\t\tif axon_num % 2 == 0:\n",
        "\t\t\tfor neuron in range(len(axon)):\n",
        "\t\t\t\tret_array[axon_num][neuron] = 1\n",
        "\t\telse:\n",
        "\t\t\tfor neuron in range(len(axon)):\n",
        "\t\t\t\tret_array[axon_num][neuron] = -1\n",
        "\n",
        "\treturn tf.convert_to_tensor(ret_array)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNT1woy81Uuz"
      },
      "source": [
        "# Contains the code for the additive pooling layer required by a Tea Layer when using TeaLearning.\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "\n",
        "# Future Calls:\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# Imports:\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# From Imports:\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\"\"\"\n",
        "Additive Pooling Class:\n",
        "    A helper layer designed to format data for output during the TeaLarning process.\n",
        "\n",
        "    If the input data to the layer has multiple spikers per classification, then for each\n",
        "    tick the spikes must be summed up. Then, once all neurons that correspond to a certain class\n",
        "    have finished spiking, their sums will dictate the results for each class.\n",
        "\n",
        "    Neurons are assumed to be arracnged such that each [num_class] represents a guess for each\n",
        "    of the classes.\n",
        "\n",
        "    For example:\n",
        "        If we have 10 classes, and we are using 250 neurons. Then we would have something like:\n",
        "        neuron_number: 0    1   2   3   4   5   6   7   8   9   10  11  12  13  ...\n",
        "        class:         0    1   2   3   4   5   6   7   8   9   0   1   2   3   ...\n",
        "\"\"\"\n",
        "######################################################################################################################################\n",
        "class AdditivePooling(layers.Layer):\n",
        "    \"\"\"\n",
        "    Initializer for new AdditivePooling Layer.\n",
        "\n",
        "    Arguments:\n",
        "        num_classes -- The number of classes to be output'd\n",
        "    \"\"\"\n",
        "    ##################################################################################################################################\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 use_additive_pooling_processing=False,\n",
        "                 add_pool_process_max=128,\n",
        "                 **kwargs):\n",
        "        super(AdditivePooling, self).__init__(**kwargs)\n",
        "\n",
        "        self.num_classes=num_classes\n",
        "        \n",
        "        self.num_inputs=None\n",
        "        \n",
        "        self.use_additive_pooling_processing=use_additive_pooling_processing\n",
        "        \n",
        "        self.add_pool_process_max=add_pool_process_max\n",
        "    ##################################################################################################################################\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "\n",
        "        # The number of neurons must be collapsable into the number of classes.\n",
        "        # i.e if we have 10 classes, the number of neurons must be a divisor of 10.\n",
        "        assert input_shape[-1] % self.num_classes == 0\n",
        "\n",
        "        self.num_inputs = input_shape[-1]\n",
        "    ##################################################################################################################################\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.shape) >= 3:\n",
        "            output = tf.keras.backend.sum(inputs, axis=1)\n",
        "        else:\n",
        "            output = inputs\n",
        "\n",
        "        # Reshape the outputs:\n",
        "        output = tf.reshape(output, [-1, int(self.num_inputs/self.num_classes), self.num_classes])\n",
        "\n",
        "        # Sum up the neurons\n",
        "        output = tf.math.reduce_sum(output, 1)\n",
        "\n",
        "        if self.use_additive_pooling_processing:\n",
        "            # Scale the ouputs between 0 and add_pool_process_max:\n",
        "            max_val = tf.constant(self.add_pool_process_max, dtype=tf.float32)\n",
        "            max_output = tf.stack([tf.reduce_max(output,1) for i in range(self.num_classes)], axis=1)\n",
        "            max_output = tf.math.divide(max_val, max_output)\n",
        "            output = tf.math.multiply(output, max_output)\n",
        "\n",
        "            # Convert any NaN's to 0:\n",
        "            output = tf.where(tf.math.is_nan(output), tf.zeros_like(output), output)\n",
        "\n",
        "        return output\n",
        "    ##################################################################################################################################\n",
        "    def computer_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "\n",
        "        # Last dimension will be number of classes:\n",
        "        output_shape[-1] = self.num_classes\n",
        "\n",
        "        # Ticks were summed, so delete tick dimension if they exist:\n",
        "        if len(output_shape) >= 3:\n",
        "            del output_shape[1]\n",
        "\n",
        "        return tuple(output_shape)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9ZU_VMh2Bm7"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras import activations, losses\n",
        "from keras.layers import Conv2D, Flatten, Input, Activation, Dense, Dropout\n",
        "from keras.utils import np_utils\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from skimage.filters import threshold_otsu\n",
        " \n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "for i in np.arange(len(x_train)):\n",
        "  thresh = threshold_otsu(x_train[i])\n",
        "  x_train[i] = x_train[i] > thresh\n",
        "\n",
        "for i in np.arange(len(x_test)):\n",
        "  thresh = threshold_otsu(x_test[i])\n",
        "  x_test[i] = x_test[i] > thresh\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM1UT1Wc2JcS"
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "#x_train /= 255\n",
        "#x_test /= 255\n",
        "            \n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO0_3Kmq94-U",
        "outputId": "bb07decd-9cfb-4f50-8b4f-6f26d973b698"
      },
      "source": [
        "\n",
        "inputs = Input(shape=(28,28,1))\n",
        "\n",
        "#network = Conv2D(10, (5,5), name='conv2d_this', activation=activations.relu)(inputs)\n",
        "\n",
        "network2 = Conv2D(15, (11,11), name='cwt', activation=activations.relu)(inputs)\n",
        "\n",
        "drop = Dropout(0.0)(network2)\n",
        "\n",
        "flattened = Flatten()(drop)\n",
        "\n",
        "dense = Tea(units=120, name=\"tea_2\")(flattened)\n",
        "\n",
        "pooling = AdditivePooling(10)(dense)\n",
        "\n",
        "predictions = Activation('softmax')(pooling)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) \n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=25, verbose=1, validation_split=0.2)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Test Loss: \", score[0]) \n",
        "print(\"Test Accuracy: \", score[1])\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "375/375 [==============================] - 5s 5ms/step - loss: 0.7465 - accuracy: 0.7574 - val_loss: 0.1705 - val_accuracy: 0.9477\n",
            "Epoch 2/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1361 - accuracy: 0.9580 - val_loss: 0.1186 - val_accuracy: 0.9647\n",
            "Epoch 3/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0918 - accuracy: 0.9726 - val_loss: 0.1109 - val_accuracy: 0.9673\n",
            "Epoch 4/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0723 - accuracy: 0.9784 - val_loss: 0.0934 - val_accuracy: 0.9737\n",
            "Epoch 5/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0601 - accuracy: 0.9827 - val_loss: 0.0866 - val_accuracy: 0.9746\n",
            "Epoch 6/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0480 - accuracy: 0.9869 - val_loss: 0.0834 - val_accuracy: 0.9739\n",
            "Epoch 7/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0418 - accuracy: 0.9886 - val_loss: 0.0775 - val_accuracy: 0.9775\n",
            "Epoch 8/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0410 - accuracy: 0.9879 - val_loss: 0.0743 - val_accuracy: 0.9769\n",
            "Epoch 9/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0346 - accuracy: 0.9900 - val_loss: 0.0741 - val_accuracy: 0.9775\n",
            "Epoch 10/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0304 - accuracy: 0.9917 - val_loss: 0.0723 - val_accuracy: 0.9772\n",
            "Epoch 11/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0279 - accuracy: 0.9922 - val_loss: 0.0749 - val_accuracy: 0.9776\n",
            "Epoch 12/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0248 - accuracy: 0.9938 - val_loss: 0.0684 - val_accuracy: 0.9783\n",
            "Epoch 13/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9938 - val_loss: 0.0691 - val_accuracy: 0.9787\n",
            "Epoch 14/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0224 - accuracy: 0.9948 - val_loss: 0.0661 - val_accuracy: 0.9802\n",
            "Epoch 15/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9953 - val_loss: 0.0716 - val_accuracy: 0.9794\n",
            "Epoch 16/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0189 - accuracy: 0.9954 - val_loss: 0.0663 - val_accuracy: 0.9812\n",
            "Epoch 17/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9958 - val_loss: 0.0641 - val_accuracy: 0.9813\n",
            "Epoch 18/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0156 - accuracy: 0.9963 - val_loss: 0.0638 - val_accuracy: 0.9805\n",
            "Epoch 19/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0157 - accuracy: 0.9960 - val_loss: 0.0654 - val_accuracy: 0.9803\n",
            "Epoch 20/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0144 - accuracy: 0.9970 - val_loss: 0.0623 - val_accuracy: 0.9813\n",
            "Epoch 21/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0127 - accuracy: 0.9971 - val_loss: 0.0660 - val_accuracy: 0.9800\n",
            "Epoch 22/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0120 - accuracy: 0.9971 - val_loss: 0.0640 - val_accuracy: 0.9814\n",
            "Epoch 23/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.0613 - val_accuracy: 0.9823\n",
            "Epoch 24/25\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0107 - accuracy: 0.9979 - val_loss: 0.0631 - val_accuracy: 0.9818\n",
            "Epoch 25/25\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0115 - accuracy: 0.9977 - val_loss: 0.0594 - val_accuracy: 0.9832\n",
            "Test Loss:  0.055152855813503265\n",
            "Test Accuracy:  0.9847999811172485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptnv7td5ORir",
        "outputId": "b7516b89-b090-4c84-f05c-9a66b428dfd7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "cwt (Conv2D)                 (None, 18, 18, 15)        1830      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 18, 18, 15)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4860)              0         \n",
            "_________________________________________________________________\n",
            "tea_2 (Tea)                  (None, 120)               1166520   \n",
            "_________________________________________________________________\n",
            "additive_pooling (AdditivePo (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,168,350\n",
            "Trainable params: 585,150\n",
            "Non-trainable params: 583,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luJtzwzeOire",
        "outputId": "f31477d9-7932-4035-84cc-8db581fa6f82"
      },
      "source": [
        "orig_weights, constrained_weights = constrain_weights(model)\n",
        "model.set_weights(constrained_weights)\n",
        "\t\t\n",
        "\n",
        "print(\"Post-Ternary Constraint Accuracy: \")\n",
        "\n",
        "# Evaluate the constained weight model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "test_predictions = model.predict(x_test)\n",
        "\n",
        "print(\"Test Loss: \", score[0])\n",
        "print(\"Test Accuracy: \", score[1])\n",
        "\n",
        "# Restore the original weights temporarily so we can evaluate them\n",
        "model.set_weights(orig_weights)\n",
        "\n",
        "print(\"Original Floating-Point Accuracy: \")\n",
        "\n",
        "# Evaluate the original, floating-point weight model\n",
        "float_score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Test Loss: \", float_score[0])\n",
        "print(\"Test Accuracy: \", float_score[1])\n",
        "\n",
        "print(\"Accuracy loss due to train-then-constrain: \" ,float_score[1] - score[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Post-Ternary Constraint Accuracy: \n",
            "Test Loss:  0.18982550501823425\n",
            "Test Accuracy:  0.9502000212669373\n",
            "Original Floating-Point Accuracy: \n",
            "Test Loss:  0.055152855813503265\n",
            "Test Accuracy:  0.9847999811172485\n",
            "Accuracy loss due to train-then-constrain:  0.03459995985031128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0uHEo6WZFwT",
        "outputId": "6d7e3f60-7ea6-4533-b08b-3bbfe32fd852"
      },
      "source": [
        "inputsCWT = Input(shape=(28,28,1))\n",
        "\n",
        "#network = Conv2D(10, (5,5), name='conv2d_this', activation=activations.relu)(inputs)\n",
        "\n",
        "network2CWT = CWTConv2D(filters=15,\n",
        "\t\t\t\t  kernel_size=(11,11),\n",
        "\t\t\t\t  strides=(1,1),\n",
        "\t\t\t\t  #activation='relu',\n",
        "\t\t\t\t  #kernel_regularizer=regularizers.l1(l=0.1),\n",
        "\t\t\t\t  use_bias=True\n",
        "\t\t\t\t  )(inputsCWT)\n",
        "      \n",
        "drop = Dropout(0.5)(network2CWT)\n",
        "\n",
        "flattenedCWT = Flatten()(drop)\n",
        "\n",
        "tea = Tea(units=120, name=\"tea_2\")(flattenedCWT)\n",
        "\n",
        "poolingCWT = AdditivePooling(10)(tea)\n",
        "\n",
        "predictionsCWT = Activation('softmax')(poolingCWT)\n",
        "\n",
        "modelCWT = Model(inputs=inputsCWT, outputs=predictionsCWT)\n",
        "\n",
        "modelCWT.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) \n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "modelCWT.fit(x_train, y_train, batch_size=128, epochs=100, verbose=1, validation_split=0.2)\n",
        "\n",
        "scoreCWT = modelCWT.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Test Loss: \", scoreCWT[0]) \n",
        "print(\"Test Accuracy: \", scoreCWT[1])\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.3978 - accuracy: 0.5543 - val_loss: 0.4766 - val_accuracy: 0.8539\n",
            "Epoch 2/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5509 - accuracy: 0.8402 - val_loss: 0.3813 - val_accuracy: 0.8902\n",
            "Epoch 3/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.4512 - accuracy: 0.8699 - val_loss: 0.3246 - val_accuracy: 0.9012\n",
            "Epoch 4/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3931 - accuracy: 0.8865 - val_loss: 0.2972 - val_accuracy: 0.9096\n",
            "Epoch 5/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3386 - accuracy: 0.9037 - val_loss: 0.2614 - val_accuracy: 0.9208\n",
            "Epoch 6/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3085 - accuracy: 0.9143 - val_loss: 0.2425 - val_accuracy: 0.9277\n",
            "Epoch 7/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2841 - accuracy: 0.9167 - val_loss: 0.2231 - val_accuracy: 0.9323\n",
            "Epoch 8/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2537 - accuracy: 0.9281 - val_loss: 0.2076 - val_accuracy: 0.9398\n",
            "Epoch 9/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2342 - accuracy: 0.9336 - val_loss: 0.1989 - val_accuracy: 0.9398\n",
            "Epoch 10/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2252 - accuracy: 0.9342 - val_loss: 0.1848 - val_accuracy: 0.9447\n",
            "Epoch 11/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2027 - accuracy: 0.9427 - val_loss: 0.1745 - val_accuracy: 0.9472\n",
            "Epoch 12/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1832 - accuracy: 0.9501 - val_loss: 0.1607 - val_accuracy: 0.9516\n",
            "Epoch 13/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1764 - accuracy: 0.9493 - val_loss: 0.1660 - val_accuracy: 0.9500\n",
            "Epoch 14/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1717 - accuracy: 0.9513 - val_loss: 0.1529 - val_accuracy: 0.9529\n",
            "Epoch 15/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1618 - accuracy: 0.9548 - val_loss: 0.1534 - val_accuracy: 0.9537\n",
            "Epoch 16/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1491 - accuracy: 0.9596 - val_loss: 0.1450 - val_accuracy: 0.9560\n",
            "Epoch 17/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1399 - accuracy: 0.9609 - val_loss: 0.1424 - val_accuracy: 0.9552\n",
            "Epoch 18/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1383 - accuracy: 0.9600 - val_loss: 0.1373 - val_accuracy: 0.9604\n",
            "Epoch 19/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1288 - accuracy: 0.9642 - val_loss: 0.1343 - val_accuracy: 0.9602\n",
            "Epoch 20/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1274 - accuracy: 0.9641 - val_loss: 0.1277 - val_accuracy: 0.9615\n",
            "Epoch 21/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.1251 - accuracy: 0.9644 - val_loss: 0.1283 - val_accuracy: 0.9622\n",
            "Epoch 22/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1209 - accuracy: 0.9656 - val_loss: 0.1448 - val_accuracy: 0.9578\n",
            "Epoch 23/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1125 - accuracy: 0.9677 - val_loss: 0.1322 - val_accuracy: 0.9618\n",
            "Epoch 24/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.1126 - accuracy: 0.9678 - val_loss: 0.1326 - val_accuracy: 0.9600\n",
            "Epoch 25/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1063 - accuracy: 0.9695 - val_loss: 0.1291 - val_accuracy: 0.9604\n",
            "Epoch 26/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.1057 - accuracy: 0.9701 - val_loss: 0.1265 - val_accuracy: 0.9632\n",
            "Epoch 27/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0986 - accuracy: 0.9725 - val_loss: 0.1290 - val_accuracy: 0.9614\n",
            "Epoch 28/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0996 - accuracy: 0.9723 - val_loss: 0.1169 - val_accuracy: 0.9658\n",
            "Epoch 29/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1004 - accuracy: 0.9713 - val_loss: 0.1273 - val_accuracy: 0.9588\n",
            "Epoch 30/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0928 - accuracy: 0.9738 - val_loss: 0.1203 - val_accuracy: 0.9650\n",
            "Epoch 31/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0936 - accuracy: 0.9724 - val_loss: 0.1219 - val_accuracy: 0.9650\n",
            "Epoch 32/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0910 - accuracy: 0.9736 - val_loss: 0.1218 - val_accuracy: 0.9635\n",
            "Epoch 33/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0872 - accuracy: 0.9762 - val_loss: 0.1244 - val_accuracy: 0.9630\n",
            "Epoch 34/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0873 - accuracy: 0.9751 - val_loss: 0.1150 - val_accuracy: 0.9647\n",
            "Epoch 35/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0878 - accuracy: 0.9746 - val_loss: 0.1058 - val_accuracy: 0.9693\n",
            "Epoch 36/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0840 - accuracy: 0.9757 - val_loss: 0.1142 - val_accuracy: 0.9661\n",
            "Epoch 37/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0828 - accuracy: 0.9767 - val_loss: 0.1240 - val_accuracy: 0.9644\n",
            "Epoch 38/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0816 - accuracy: 0.9769 - val_loss: 0.1057 - val_accuracy: 0.9691\n",
            "Epoch 39/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0841 - accuracy: 0.9763 - val_loss: 0.1187 - val_accuracy: 0.9653\n",
            "Epoch 40/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0782 - accuracy: 0.9777 - val_loss: 0.1135 - val_accuracy: 0.9665\n",
            "Epoch 41/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0790 - accuracy: 0.9783 - val_loss: 0.1060 - val_accuracy: 0.9682\n",
            "Epoch 42/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0721 - accuracy: 0.9793 - val_loss: 0.1043 - val_accuracy: 0.9708\n",
            "Epoch 43/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0735 - accuracy: 0.9803 - val_loss: 0.1015 - val_accuracy: 0.9683\n",
            "Epoch 44/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0759 - accuracy: 0.9785 - val_loss: 0.1016 - val_accuracy: 0.9702\n",
            "Epoch 45/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0743 - accuracy: 0.9777 - val_loss: 0.1020 - val_accuracy: 0.9708\n",
            "Epoch 46/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0752 - accuracy: 0.9794 - val_loss: 0.0987 - val_accuracy: 0.9708\n",
            "Epoch 47/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0713 - accuracy: 0.9788 - val_loss: 0.1001 - val_accuracy: 0.9707\n",
            "Epoch 48/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0726 - accuracy: 0.9792 - val_loss: 0.0998 - val_accuracy: 0.9699\n",
            "Epoch 49/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0716 - accuracy: 0.9797 - val_loss: 0.0895 - val_accuracy: 0.9748\n",
            "Epoch 50/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0687 - accuracy: 0.9801 - val_loss: 0.0984 - val_accuracy: 0.9709\n",
            "Epoch 51/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0637 - accuracy: 0.9820 - val_loss: 0.0880 - val_accuracy: 0.9741\n",
            "Epoch 52/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0674 - accuracy: 0.9809 - val_loss: 0.0923 - val_accuracy: 0.9734\n",
            "Epoch 53/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0688 - accuracy: 0.9804 - val_loss: 0.0913 - val_accuracy: 0.9734\n",
            "Epoch 54/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0660 - accuracy: 0.9810 - val_loss: 0.0884 - val_accuracy: 0.9740\n",
            "Epoch 55/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0637 - accuracy: 0.9827 - val_loss: 0.0872 - val_accuracy: 0.9728\n",
            "Epoch 56/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0649 - accuracy: 0.9817 - val_loss: 0.0921 - val_accuracy: 0.9736\n",
            "Epoch 57/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0622 - accuracy: 0.9823 - val_loss: 0.0854 - val_accuracy: 0.9745\n",
            "Epoch 58/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0625 - accuracy: 0.9828 - val_loss: 0.0899 - val_accuracy: 0.9733\n",
            "Epoch 59/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0634 - accuracy: 0.9815 - val_loss: 0.0872 - val_accuracy: 0.9733\n",
            "Epoch 60/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0614 - accuracy: 0.9815 - val_loss: 0.0872 - val_accuracy: 0.9736\n",
            "Epoch 61/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0600 - accuracy: 0.9831 - val_loss: 0.0795 - val_accuracy: 0.9753\n",
            "Epoch 62/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0615 - accuracy: 0.9826 - val_loss: 0.0784 - val_accuracy: 0.9766\n",
            "Epoch 63/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0591 - accuracy: 0.9829 - val_loss: 0.0843 - val_accuracy: 0.9752\n",
            "Epoch 64/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0585 - accuracy: 0.9836 - val_loss: 0.0874 - val_accuracy: 0.9728\n",
            "Epoch 65/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0586 - accuracy: 0.9836 - val_loss: 0.0899 - val_accuracy: 0.9742\n",
            "Epoch 66/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0595 - accuracy: 0.9830 - val_loss: 0.0819 - val_accuracy: 0.9762\n",
            "Epoch 67/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0558 - accuracy: 0.9837 - val_loss: 0.0884 - val_accuracy: 0.9734\n",
            "Epoch 68/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0592 - accuracy: 0.9820 - val_loss: 0.0798 - val_accuracy: 0.9774\n",
            "Epoch 69/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0563 - accuracy: 0.9850 - val_loss: 0.0798 - val_accuracy: 0.9762\n",
            "Epoch 70/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0570 - accuracy: 0.9824 - val_loss: 0.0729 - val_accuracy: 0.9783\n",
            "Epoch 71/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0563 - accuracy: 0.9845 - val_loss: 0.0804 - val_accuracy: 0.9754\n",
            "Epoch 72/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0536 - accuracy: 0.9857 - val_loss: 0.0702 - val_accuracy: 0.9780\n",
            "Epoch 73/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0562 - accuracy: 0.9841 - val_loss: 0.0715 - val_accuracy: 0.9769\n",
            "Epoch 74/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0530 - accuracy: 0.9854 - val_loss: 0.0739 - val_accuracy: 0.9776\n",
            "Epoch 75/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0540 - accuracy: 0.9848 - val_loss: 0.0782 - val_accuracy: 0.9771\n",
            "Epoch 76/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0511 - accuracy: 0.9855 - val_loss: 0.0734 - val_accuracy: 0.9778\n",
            "Epoch 77/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0544 - accuracy: 0.9840 - val_loss: 0.0770 - val_accuracy: 0.9772\n",
            "Epoch 78/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0516 - accuracy: 0.9856 - val_loss: 0.0742 - val_accuracy: 0.9785\n",
            "Epoch 79/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0543 - accuracy: 0.9842 - val_loss: 0.0717 - val_accuracy: 0.9787\n",
            "Epoch 80/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0540 - accuracy: 0.9851 - val_loss: 0.0697 - val_accuracy: 0.9803\n",
            "Epoch 81/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0509 - accuracy: 0.9858 - val_loss: 0.0791 - val_accuracy: 0.9768\n",
            "Epoch 82/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0509 - accuracy: 0.9853 - val_loss: 0.0735 - val_accuracy: 0.9778\n",
            "Epoch 83/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0507 - accuracy: 0.9857 - val_loss: 0.0700 - val_accuracy: 0.9801\n",
            "Epoch 84/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0500 - accuracy: 0.9859 - val_loss: 0.0666 - val_accuracy: 0.9797\n",
            "Epoch 85/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0478 - accuracy: 0.9875 - val_loss: 0.0727 - val_accuracy: 0.9780\n",
            "Epoch 86/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0488 - accuracy: 0.9859 - val_loss: 0.0684 - val_accuracy: 0.9804\n",
            "Epoch 87/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0484 - accuracy: 0.9859 - val_loss: 0.0703 - val_accuracy: 0.9778\n",
            "Epoch 88/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0498 - accuracy: 0.9853 - val_loss: 0.0729 - val_accuracy: 0.9783\n",
            "Epoch 89/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0509 - accuracy: 0.9855 - val_loss: 0.0718 - val_accuracy: 0.9782\n",
            "Epoch 90/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0469 - accuracy: 0.9866 - val_loss: 0.0779 - val_accuracy: 0.9760\n",
            "Epoch 91/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0496 - accuracy: 0.9859 - val_loss: 0.0756 - val_accuracy: 0.9775\n",
            "Epoch 92/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0472 - accuracy: 0.9857 - val_loss: 0.0736 - val_accuracy: 0.9781\n",
            "Epoch 93/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0464 - accuracy: 0.9876 - val_loss: 0.0751 - val_accuracy: 0.9783\n",
            "Epoch 94/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0456 - accuracy: 0.9878 - val_loss: 0.0767 - val_accuracy: 0.9773\n",
            "Epoch 95/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0448 - accuracy: 0.9876 - val_loss: 0.0734 - val_accuracy: 0.9776\n",
            "Epoch 96/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0464 - accuracy: 0.9868 - val_loss: 0.0663 - val_accuracy: 0.9803\n",
            "Epoch 97/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0442 - accuracy: 0.9881 - val_loss: 0.0756 - val_accuracy: 0.9769\n",
            "Epoch 98/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0436 - accuracy: 0.9872 - val_loss: 0.0751 - val_accuracy: 0.9782\n",
            "Epoch 99/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0448 - accuracy: 0.9880 - val_loss: 0.0734 - val_accuracy: 0.9785\n",
            "Epoch 100/100\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.0440 - accuracy: 0.9878 - val_loss: 0.0675 - val_accuracy: 0.9797\n",
            "Test Loss:  0.05711401253938675\n",
            "Test Accuracy:  0.9817000031471252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jTwboT9bTuN",
        "outputId": "cfafa09e-85b0-447f-a74e-e2b1c1314656"
      },
      "source": [
        "orig_weightsCWT, constrained_weightsCWT = constrain_weights_cwt(modelCWT)\n",
        "modelCWT.set_weights(constrained_weightsCWT)\n",
        "\t\t\n",
        "\n",
        "print(\"Post-Ternary Constraint Accuracy: \")\n",
        "\n",
        "# Evaluate the constained weight model\n",
        "scoreCWT = modelCWT.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "test_predictionsCWT = modelCWT.predict(x_test)\n",
        "\n",
        "print(\"Test Loss: \", scoreCWT[0])\n",
        "print(\"Test Accuracy: \", scoreCWT[1])\n",
        "\n",
        "# Restore the original weights temporarily so we can evaluate them\n",
        "modelCWT.set_weights(orig_weightsCWT)\n",
        "\n",
        "print(\"Original Floating-Point Accuracy: \")\n",
        "\n",
        "# Evaluate the original, floating-point weight model\n",
        "float_scoreCWT = modelCWT.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Test Loss: \", float_scoreCWT[0])\n",
        "print(\"Test Accuracy: \", float_scoreCWT[1])\n",
        "\n",
        "print(\"Accuracy loss due to train-then-constrain: \" ,float_scoreCWT[1] - scoreCWT[1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Post-Ternary Constraint Accuracy: \n",
            "Test Loss:  0.05711401253938675\n",
            "Test Accuracy:  0.9817000031471252\n",
            "Original Floating-Point Accuracy: \n",
            "Test Loss:  0.05711401253938675\n",
            "Test Accuracy:  0.9817000031471252\n",
            "Accuracy loss due to train-then-constrain:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}