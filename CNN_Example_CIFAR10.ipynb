{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifarbee.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlwoMAiuZBpl"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# General Imports:\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "\n",
        "# From Imports:\n",
        "from tensorflow.keras import layers, activations, initializers, regularizers, constraints, Model\n",
        "\n",
        "from skimage.filters import threshold_otsu\n",
        "\n",
        "@tf.custom_gradient\n",
        "def custom_round(x):\n",
        "\toutput = tf.keras.backend.round(x)\n",
        "\tdef grad(dy):\n",
        "\t\treturn dy#*(tf.keras.backend.maximum(0*x, 1-2*tf.keras.backend.abs(x-0.5)) + tf.keras.backend.maximum(0*x, 1-2*tf.keras.backend.abs(x+0.5)))\n",
        "\treturn output, grad\n",
        "\n",
        "class CWTConv2D(layers.Layer) :\n",
        "\t\n",
        "\tdef __init__(self, filters, kernel_size, strides, use_bias=True, **kwargs):\n",
        "\t\tsuper(CWTConv2D, self).__init__(**kwargs)\n",
        "\t\t\n",
        "\t\tself.filters = filters\n",
        "\t\tself.kernel_size = kernel_size\n",
        "\t\tself.strides = strides\n",
        "\t\tself.use_bias = use_bias\n",
        "\n",
        "\tdef build(self, input_shape):\n",
        "\t\t\n",
        "\t\tself.kernel = self.add_weight(name='kernel',\n",
        "\t\t\t\t\t\t\t\t\t\tshape=(self.kernel_size[0], self.kernel_size[1], input_shape[-1], self.filters),\n",
        "\t\t\t\t\t\t\t\t\t\tinitializer=initializers.RandomNormal(mean=0.0, stddev=0.5, seed=0),\n",
        "\t\t\t\t\t\t\t\t\t\ttrainable=True)\n",
        "\n",
        "\t\tif (self.use_bias) :\t\t\t\t\t\t\t   \n",
        "\t\t\tself.biases = self.add_weight(name='biases',\n",
        "\t\t\t\t\t\t\t\t\t\t\tshape=(self.filters),\n",
        "\t\t\t\t\t\t\t\t\t\t\tinitializer='zeros',\n",
        "\t\t\t\t\t\t\t\t\t\t\ttrainable=True)\n",
        "\t\t\t\t\t\t\t\t\t\t  \n",
        "\t\tsuper(CWTConv2D, self).build(input_shape)\n",
        "\t\t\t\t\t\t\t\t\t\t  \n",
        "\tdef call(self, inputs):\n",
        "\t\t\n",
        "\t\tinputs = custom_round(inputs)\n",
        "\t\t\n",
        "\t\tkernel = self.kernel\n",
        "\t\t\n",
        "\t\t#kernel = tf.keras.backend.round(kernel)\n",
        "\t\tkernel = custom_round(kernel)\n",
        "\t\t\n",
        "\t\tkernel = tf.keras.backend.clip(kernel, -1, 1)\n",
        "\t\t\n",
        "\t\toutput = tf.keras.backend.conv2d(inputs, kernel=kernel, strides=self.strides, padding='valid', data_format=\"channels_last\")\n",
        "\t\t\n",
        "\t\tif (self.use_bias) :\n",
        "\t\t\tbiases = self.biases\n",
        "\t\t\t\n",
        "\t\t\t#biases = tf.keras.backend.round(biases)\n",
        "\t\t\tbiases = custom_round(biases)\n",
        "\t\t\t\n",
        "\t\t\tbiases = tf.keras.backend.clip(biases, -128, 127)\n",
        "\t\t\t\n",
        "\t\t\toutput = tf.keras.backend.bias_add(output, biases, data_format='channels_last')\n",
        "\t\t\n",
        "\t\toutput = tf.keras.backend.in_train_phase(activations.sigmoid(output), tf.dtypes.cast(tf.math.greater_equal(output, 0.0), tf.float32))\n",
        "\t\t\n",
        "\t\t#output = tf.keras.backend.clip(output, 0, 1)\n",
        "\t\t\n",
        "\t\t#output = tf.keras.backend.round(output)\n",
        "\t\t#output = custom_round(output)\n",
        "\t\t\n",
        "\t\treturn output\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t \n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\t\n",
        "\t\treturn ((inputs.shape[0] - self.kernel_size[0])/self.strides[0] + 1, (inputs.shape[1] - self.kernel_size[1])/self.strides[1] + 1, self.filters)\n",
        "\t\n",
        "class SpikeData(layers.Layer) :\n",
        "\t\n",
        "\tdef __init__(self, **kwargs):\n",
        "\t\tsuper(CWTConv2D, self).__init__(**kwargs)\n",
        "\t\t\n",
        "\n",
        "\tdef build(self, input_shape):\n",
        "\t\t\t\t\t\t\t\t\t\t  \n",
        "\t\tsuper(CWTConv2D, self).build(input_shape)\n",
        "\t\t\t\t\t\t\t\t\t\t  \n",
        "\tdef call(self, inputs):\n",
        "\n",
        "\t\toutput = custom_round(inputs)\n",
        "\t\t\n",
        "\t\toutput = tf.keras.backend.clip(output, 0, 1)\n",
        "\t\t\n",
        "\t\treturn output\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t \n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\t\n",
        "\t\treturn input_shape.shape\n",
        "\t\n",
        "class CWTMaxPooling2D(layers.Layer) :\n",
        "\t\n",
        "\tdef __init__(self, pool_size, strides, **kwargs):\n",
        "\t\tsuper(CWTMaxPooling2D, self).__init__(**kwargs)\n",
        "\t\t\n",
        "\t\tself.pool_size = pool_size\n",
        "\t\tself.strides = strides\n",
        "\t\t# only supports valid padding\n",
        "\n",
        "\tdef build(self, input_shape):\n",
        "\t\t\t\t\t\t\t\t\t\t  \n",
        "\t\tsuper(CWTMaxPooling2D, self).build(input_shape)\n",
        "\t\t\t\t\t\t\t\t\t\t  \n",
        "\tdef call(self, inputs):\n",
        "\t\t\n",
        "\t\toutput = custom_round(inputs)\n",
        "\t\t\n",
        "\t\toutput = tf.keras.backend.pool2d(output, pool_size=self.pool_size, strides=self.strides, padding='valid', data_format=\"channels_last\")\n",
        "\t\t\n",
        "\t\treturn output\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t \n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\t\n",
        "\t\treturn ((inputs.shape[0] - self.pool_size[0])/self.strides[0] + 1, (inputs.shape[1] - self.pool_size[1])/self.strides[1] + 1, self.filters)\n",
        "\t\n",
        "\t\n",
        "# custom cwt constrain function\n",
        "def constrain_weights_cwt(model) :\n",
        "\n",
        "\t# Slam the convolutional kernel weights to -1, 0, or 1\n",
        "\tweights = model.get_weights()\n",
        "\torig_weights = copy.deepcopy(weights)\n",
        "\tnames = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "\t\n",
        "\tlayer_num = -1\n",
        "\tfor weight, name in zip(weights, names):\n",
        "\t\tlayer_num += 1\n",
        "\n",
        "\t\tif 'cwt' in name and 'kernel' in name:\n",
        "\t\t\tweight = tf.keras.backend.round(weight)\n",
        "\t\t\tweight = tf.keras.backend.clip(weight, -1, 1)\n",
        "\t\t\tweights[layer_num] = weight\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\tif 'cwt' in name and 'bias' in name:\t\n",
        "\t\t\tweight = tf.keras.backend.round(weight)\n",
        "\t\t\tweight = tf.keras.backend.clip(weight, -128, 127)\n",
        "\t\t\tweights[layer_num] = weight\n",
        "\t\t\t\n",
        "\treturn orig_weights, weights\n",
        "\t\n",
        "# rounds a Keras Conv2D layer's weights to a ternary value (-1, 0, 1) to \n",
        "# be compatible with deployment onto a SNN based simulator/enviornment\n",
        "# finds the average weight, and then uses the mean as a reference point for where it should go\n",
        "# this should be deprecated once the train while constrain approach is implemented\n",
        "def constrain_weights(model) :\n",
        "\n",
        "\t# Slam the convolutional kernel weights to -1, 0, or 1\n",
        "\tweights = model.get_weights()\n",
        "\torig_weights = copy.deepcopy(weights)\n",
        "\tnames = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "\t\n",
        "\tlayer_num = -1\n",
        "\tfor weight, name in zip(weights, names):\n",
        "\t\tlayer_num += 1\n",
        "\n",
        "\t\tif 'kernel' in name:\t\n",
        "\t\t\told_shape = weight.shape\n",
        "\t\t\tflattened_weights = weight.flatten()\n",
        "\t\t\tabsolute_weights = np.absolute(flattened_weights)\n",
        "\t\t\tmean = np.mean(absolute_weights)\n",
        "\t\t\tfor i in range(len(flattened_weights)) :\n",
        "\t\t\t\tif (flattened_weights[i] > mean) :\n",
        "\t\t\t\t\tflattened_weights[i] = 1\n",
        "\t\t\t\telif (flattened_weights[i] < -1 * mean) :\n",
        "\t\t\t\t\tflattened_weights[i] = -1\n",
        "\t\t\t\telse :\n",
        "\t\t\t\t\tflattened_weights[i] = 0\n",
        "\t\t\tweight = flattened_weights.reshape(old_shape)\n",
        "\t\t\tweights[layer_num] = weight\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\tif 'bias' in name:\t\n",
        "\t\t\told_shape = weight.shape\n",
        "\t\t\tflattened_weights = weight.flatten()\n",
        "\t\t\tabsolute_weights = np.absolute(flattened_weights)\n",
        "\t\t\tmean = np.mean(absolute_weights)\n",
        "\t\t\tfor i in range(len(flattened_weights)) :\n",
        "\t\t\t\tif (flattened_weights[i] > mean) :\n",
        "\t\t\t\t\tflattened_weights[i] = 1\n",
        "\t\t\t\telif (flattened_weights[i] < -1 * mean) :\n",
        "\t\t\t\t\tflattened_weights[i] = -1\n",
        "\t\t\t\telse :\n",
        "\t\t\t\t\tflattened_weights[i] = 0\n",
        "\t\t\tweight = flattened_weights.reshape(old_shape)\n",
        "\t\t\tweights[layer_num] = weight\n",
        "\t\t\t\n",
        "\treturn orig_weights, weights\t\n",
        "\t\n",
        "def print_weights(model) :\n",
        "\tview_weights = model.get_weights()\n",
        "\tnames = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "\tfor weight, name in zip(view_weights, names) :\n",
        "\t\tif 'kernel' in name:\t\n",
        "\t\t\tprint(name + \" weights: \")\n",
        "\t\t\tprint(weight.shape)\n",
        "\t\t\tprint_weights = weight\n",
        "\t\t\t\n",
        "\t\n",
        "\tfor f in range(print_weights.shape[3]) :\n",
        "\t\tfor y in range(print_weights.shape[0]) :\n",
        "\t\t\tline = \"\"\n",
        "\t\t\tfor x in range(print_weights.shape[1]) :\n",
        "\t\t\t\tfor c in range (print_weights.shape[2]) :\n",
        "\t\t\t\t\tline += str(print_weights[y][x][c][f])\n",
        "\t\t\t\tline += \" \"\n",
        "\t\t\tprint(line)\n",
        "\t\tprint(\"Next Filter: \")\n",
        "\t\t\n",
        "\tfor weight, name in zip(view_weights, names) :\n",
        "\t\tif 'bias' in name:\t\n",
        "\t\t\tprint(name + \" weights : \")\n",
        "\t\t\tprint(weight)\n",
        "\t\t\t\n",
        "\"\"\"\n",
        "takes as input a numpy array of 2d color images, and applies optimal \n",
        "color thresholding to them, transforming them to binary images\n",
        "\"\"\"\n",
        "def opt_thresh_color(x_in):\n",
        "\n",
        "\tx_bin = np.zeros_like(x_in[:,:,:,0])\n",
        "\tx = np.copy(x_in)\n",
        "\tfor i in np.arange(len(x)):\n",
        "\t\tfor color in np.arange(3):\n",
        "\t\t\tthresh = threshold_otsu(x[i,:,:,color])\n",
        "\t\t\tx[i,:,:,color] = x[i,:,:,color] > thresh\n",
        "\t\tx_bin[i] = np.logical_or(np.logical_or(np.logical_and(x[i,:,:,0],x[i,:,:,1]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t   np.logical_and(x[i,:,:,1],x[i,:,:,2])),\n",
        "\t\t\t\t\t\t\t\t np.logical_and(x[i,:,:,0],x[i,:,:,2]))\n",
        "\n",
        "\treturn x_bin\n",
        "\t\n",
        "\"\"\"\n",
        "takes as input a numpy array of 2d color images, and applies optimal \n",
        "color thresholding to them, transforming them to binary images\n",
        "\"\"\"\n",
        "def opt_thresh_color_three(x_in):\n",
        "\n",
        "\tx_bin = np.zeros_like(x_in[:,:,:,0])\n",
        "\tx = np.copy(x_in)\n",
        "\tfor i in np.arange(len(x)):\n",
        "\t\tfor color in np.arange(3):\n",
        "\t\t\tthresh = threshold_otsu(x[i,:,:,color])\n",
        "\t\t\tx[i,:,:,color] = x[i,:,:,color] > thresh\n",
        "\treturn x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGWb0MlJ3yTU"
      },
      "source": [
        "# Contains the Code for Tea Layer for use in TeaLarning and TensorFlow 2.0\n",
        "\n",
        "\n",
        "# Future Imports:\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# General Imports:\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# From Imports:\n",
        "from tensorflow.keras import layers, activations, initializers, regularizers, constraints\n",
        "#from tensorflow.keras import backend as K\n",
        "# Custom Round Gradient:\n",
        "# In a TeaLayer the connections are rounded during the feedforward process.\n",
        "# We need a custom rounding function to implement this.\n",
        "# For this function the gradient is treated as a [ 1 ] so as not to effect backprop.\n",
        "\n",
        "@tf.custom_gradient\n",
        "def CustomRound(x):\n",
        "\toutput = tf.keras.backend.round(x)\n",
        "\tdef grad(dy):\n",
        "\t\treturn dy\n",
        "\treturn output, grad\n",
        "######################################################################################################################################\n",
        "class Tea(layers.Layer):\n",
        "\t\"\"\"\n",
        "\tThe following is an implementation of a TeaLayer used to implement IBM's TeaLarning Training\n",
        "\t\tmethod for RANC-based TrueNorth deployment.\n",
        "\tFor this to be compatable with the TrueNorth architecture, some irregular constraints and \n",
        "\t\tfunctionalities must be implemented.\n",
        "\tEach layer contains stastically initialized [ weights ], which are multiplied by trainable\n",
        "\t\t[ connections ]. [ Connections ] are floating point values which represent the\n",
        "\t\tprobablity that a [ connection ] exists. When feeding-forward [ connections ] are normally\n",
        "\t\tconstrained to the values of 1, if >= 0.5, and 0, else. \n",
        "\t\tThis method allows for feed-forward to represent actual TrueNorth computations, but still\n",
        "\t\tallows connections to be trained during backprop.\n",
        "\tAdditionally, inputs into the TrueNorth layer must be constrained to binary spikes of 1 or 0.\n",
        "\t\tInput data is normalized between 0 and 1, then if the value is 0.5 or greater it is represented\n",
        "\t\tas a spike (1), otherwise it is represented as a non-spike (0).\n",
        "\tFinally, outputs of the layer must be constrained during validation and testing. After weighted\n",
        "\t\tinputs are calculated, each value is set to 1 if it is greater than or equal to 0, and 0 otherwise.\n",
        "\t\tDuring training, this is estimated by a sigmoid activation function.\n",
        "\t\"\"\"\n",
        "\n",
        "\t\"\"\"\n",
        "\tNew TeaLayer Initialization:\n",
        "\tArguments:\n",
        "\t\tunits -- The number of neurons to use for this layer.\n",
        "\tKeyword Arguments:\n",
        "\t\tactivation -- The type of activation function to use to estimate spiking during training.\n",
        "\t\t\t\t\t  Note: Sigmoid activation function is specifically chosen to most accurately\n",
        "\t\t\t\t\t\trepresent spiking. This value [should] be left as the default.\n",
        "\t\t\t\t\t  DEFAULT: [sigmoid]\n",
        "\t\tuse_bias   -- To use biases or not.\n",
        "\t\t\t\t\t  DEFAULT: [True]\n",
        "\t\tweight_initializer -- The initializer to use when initializing the weights. If [None], then\n",
        "\t\t\t\t\t\t\t  the function [tea_weight_initializer] is used. This function sets all\n",
        "\t\t\t\t\t\t\t  weights to be -1 or 1 as outlined by IBM's TeaLarning Literature.\n",
        "\t\t\t\t\t\t\t  DEFAULT: [None]\n",
        "\t\tbias_initalizer -- The initializer to use when initializing biases.\n",
        "\t\t\t\t\t\t   DEFAULT: [zeros]\n",
        "\t\tconnection_initializer -- The initializer to use when initializing connection values.\n",
        "\t\t\t\t\t\t\t\t  Note: Connections should be initialized as a [probability distribution].\n",
        "\t\t\t\t\t\t\t\t  By default they are sampled from a normal distrubtion with a mean of 0.5\n",
        "\t\t\t\t\t\t\t\t  DEFAULT: [None]\n",
        "\t\tconnection_regularizer -- A regularizer to use on the connection values, if any.\n",
        "\t\t\t\t\t\t\t\t  DEFAULT: [None]\n",
        "\t\t\n",
        "\t\tconnection_constraint -- A constraint to apply to the connections, if any.\n",
        "\t\t\t\t\t\t\t\t DEFAULT: [None]\n",
        "\t\tround_input -- When feeding-forward this option dictates to round the input values or not to. This should\n",
        "\t\t\t\t\t   be set to [True] to maintain spiking simulation.\n",
        "\t\t\t\t\t   DEFAULT: [True]\n",
        "\t\t\n",
        "\t\tround_connections -- When feeding-forward this option dictates to round the connection values or not.\n",
        "\t\t\t\t\t\t\t DEFAULT: [True]\n",
        "\t\tclip_connection -- This option dictates if connection values should be clipped between 0 and 1 when feeding-forward.\n",
        "\t\t\t\t\t\t   DEFAULT: [True]\n",
        "\t\tround_bias -- This option dictates if biases should be rounded when feeding-forward.\n",
        "\t\t\t\t\t  DEFAULT: [True]\n",
        "\t\tconstrain_after_train -- This option dictates if outputs should be constrained to spikes (0 or 1) when training is completed.\n",
        "\t\t\t\t\t\t\t\t DEFAULT: [True]\n",
        "\t\"\"\"\n",
        "\t##################################################################################################################################\n",
        "\tdef __init__(self,\n",
        "\t\t\t\t units,\n",
        "\t\t\t\t activation='sigmoid',\n",
        "\t\t\t\t use_bias=True,\n",
        "\t\t\t\t weight_initializer=None,\n",
        "\t\t\t\t bias_initializer='zeros',\n",
        "\t\t\t\t connection_initializer=None,\n",
        "\t\t\t\t connection_regularizer=None,\n",
        "\t\t\t\t connection_constraint=None,\n",
        "\t\t\t\t round_input=True,\n",
        "\t\t\t\t round_connections=True,\n",
        "\t\t\t\t clip_connections=True,\n",
        "\t\t\t\t round_bias=True,\n",
        "\t\t\t\t constrain_after_train=True,\n",
        "\t\t\t\t **kwargs):\n",
        "\t\tsuper(Tea, self).__init__(**kwargs)\n",
        "\t\t\n",
        "\t\tself.units=units\n",
        "\t\t\n",
        "\t\tself.activation=activations.get(activation)\n",
        "\t\t\n",
        "\t\tself.use_bias=use_bias\n",
        "\n",
        "\t\tif connection_initializer:\n",
        "\t\t\tself.connection_initializer=connection_initializer\n",
        "\t\telse:\n",
        "\t\t\tself.connection_initializer=initializers.TruncatedNormal(mean=0.5, seed=0)\n",
        "\n",
        "\t\tif weight_initializer:\n",
        "\t\t\tself.weight_initializer=weight_initializer\n",
        "\t\telse:\n",
        "\t\t\tself.weight_initializer=tea_weight_initializer\n",
        "\n",
        "\t\tself.bias_initializer=bias_initializer\n",
        "\n",
        "\t\tself.connection_regularizer=connection_regularizer\n",
        "\t\t\n",
        "\t\tself.connection_constraint=connection_constraint\n",
        "\n",
        "\t\tself.input_width=None\n",
        "\n",
        "\t\tself.round_input=round_input\n",
        "\n",
        "\t\tself.round_connections=round_connections\n",
        "\t\t\n",
        "\t\tself.clip_connections=clip_connections\n",
        "\n",
        "\t\tself.round_bias=round_bias\n",
        "\n",
        "\t\tself.constrain_after_train=constrain_after_train\n",
        "\n",
        "\t\tself.uses_learning_phase=True\n",
        "\t##################################################################################################################################\n",
        "\tdef build(self, input_shape):\n",
        "\t\tassert len(input_shape) >= 2\n",
        "\n",
        "\t\tsuper(Tea,self).build(input_shape)\n",
        "\n",
        "\t\tshape = (input_shape[-1], self.units)\n",
        "\n",
        "\t\tself.static_weights = self.add_weight(name='weights',\n",
        "\t\t\t\t\t\t\t\t\t\t\t  shape=shape,\n",
        "\t\t\t\t\t\t\t\t\t\t\t  initializer=self.weight_initializer,\n",
        "\t\t\t\t\t\t\t\t\t\t\t  trainable=False)\n",
        "\n",
        "\t\tself.connections = self.add_weight(name='connections',\n",
        "\t\t\t\t\t\t\t\t\t\t   shape=shape,\n",
        "\t\t\t\t\t\t\t\t\t\t   initializer=self.connection_initializer,\n",
        "\t\t\t\t\t\t\t\t\t\t   regularizer=self.connection_regularizer,\n",
        "\t\t\t\t\t\t\t\t\t\t   constraint=self.connection_constraint)\n",
        "\n",
        "\t\tif self.use_bias:\n",
        "\t\t\tself.biases = self.add_weight(name='bias',\n",
        "\t\t\t\t\t\t\t\t\t\t  shape=(self.units,),\n",
        "\t\t\t\t\t\t\t\t\t\t  initializer=self.bias_initializer)\n",
        "\t##################################################################################################################################\n",
        "\tdef call(self, inputs):\n",
        "\n",
        "\t\t# Constrain the Input:\n",
        "\t\tif self.round_input:\n",
        "\t\t\tinputs = CustomRound(inputs)\n",
        "\t\telse:\n",
        "\t\t\tinputs = tf.keras.backend.in_train_phase(inputs, CustomRound(inputs))\n",
        "\n",
        "\t\t# Connection Constraints:\n",
        "\t\tconnections = self.connections\n",
        "\n",
        "\t\tif self.round_connections:\n",
        "\t\t\tconnections = CustomRound(connections)\n",
        "\t\telse:\n",
        "\t\t\tconnections = tf.keras.backend.in_train_phase(connections, CustomRound(connections))\n",
        "\n",
        "\t\tif self.clip_connections:\n",
        "\t\t\tconnections = tf.keras.backend.clip(connections, 0, 1)\n",
        "\t\telse:\n",
        "\t\t\tconnections = tf.keras.backend.in_train_phase(connections, tf.keras.backend.clip(connections, 0, 1))\n",
        "\n",
        "\t\t# Multiply Connections with Weights:\n",
        "\t\tweighted_connections = connections * self.static_weights\n",
        "\n",
        "\t\t# Dot Product the Input with the Weighted Connections\n",
        "\t\toutput = tf.keras.backend.dot(inputs, weighted_connections)\n",
        "\n",
        "\t\t# Add biases if they are being used:\n",
        "\t\tif self.use_bias:\n",
        "\t\t\t\n",
        "\t\t\t# Constrain the biases first:\n",
        "\t\t\tif self.round_bias:\n",
        "\t\t\t\tbiases = CustomRound(self.biases)\n",
        "\t\t\telse:\n",
        "\t\t\t\tbiases = tf.keras.backend.in_train_phase(self.biases, CustomRound(self.biases))\n",
        "\n",
        "\t\t\toutput = tf.keras.backend.bias_add(output, biases, data_format='channels_last')\n",
        "\n",
        "\t\t# Apply activation / Spike(s)\n",
        "\t\toutput = tf.keras.backend.in_train_phase(self.activation(output), \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t tf.dtypes.cast(tf.math.greater_equal(output, 0.0), tf.float32))\n",
        "\n",
        "\t\treturn output\n",
        "\t##################################################################################################################################\n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\tassert input_shape and len(input_shape) >= 2\n",
        "\t\tassert input_shape[-1]\n",
        "\n",
        "\t\toutput_shape = list(input_shape)\n",
        "\n",
        "\t\toutput_shape[-1] = self.units\n",
        "\n",
        "\t\treturn tuple(output_shape)\n",
        "\t##################################################################################################################################\n",
        "# END CLASS : TEA\n",
        "\"\"\"\n",
        "This function returns a tensor of alternating 1s and -1s. This is a basic re-implementation of IBM's own weight matrix initializations.\n",
        "Argument:\n",
        "\tshape -- The shape of the weights to be initialized.\n",
        "Keyword Arguments:\n",
        "\tdtype -- The data type to be used when initializing the weights.\n",
        "\t\t\t DEFAULT : [np.float32]\n",
        "\"\"\"\n",
        "def tea_weight_initializer(shape, dtype=np.float32):\n",
        "\tnum_axons = shape[0]\n",
        "\tnum_neurons = shape[1]\n",
        "\tif dtype == 'float32':\n",
        "\t\tdtype = np.float32\n",
        "\tret_array = np.zeros((int(num_axons), int(num_neurons)), dtype=dtype)\n",
        "\n",
        "\tfor axon_num, axon in enumerate(ret_array):\n",
        "\t\tif axon_num % 2 == 0:\n",
        "\t\t\tfor neuron in range(len(axon)):\n",
        "\t\t\t\tret_array[axon_num][neuron] = 1\n",
        "\t\telse:\n",
        "\t\t\tfor neuron in range(len(axon)):\n",
        "\t\t\t\tret_array[axon_num][neuron] = -1\n",
        "\n",
        "\treturn tf.convert_to_tensor(ret_array)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNT1woy81Uuz"
      },
      "source": [
        "# Contains the code for the additive pooling layer required by a Tea Layer when using TeaLearning.\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "\n",
        "# Future Calls:\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# Imports:\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# From Imports:\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\"\"\"\n",
        "Additive Pooling Class:\n",
        "    A helper layer designed to format data for output during the TeaLarning process.\n",
        "\n",
        "    If the input data to the layer has multiple spikers per classification, then for each\n",
        "    tick the spikes must be summed up. Then, once all neurons that correspond to a certain class\n",
        "    have finished spiking, their sums will dictate the results for each class.\n",
        "\n",
        "    Neurons are assumed to be arracnged such that each [num_class] represents a guess for each\n",
        "    of the classes.\n",
        "\n",
        "    For example:\n",
        "        If we have 10 classes, and we are using 250 neurons. Then we would have something like:\n",
        "        neuron_number: 0    1   2   3   4   5   6   7   8   9   10  11  12  13  ...\n",
        "        class:         0    1   2   3   4   5   6   7   8   9   0   1   2   3   ...\n",
        "\"\"\"\n",
        "######################################################################################################################################\n",
        "class AdditivePooling(layers.Layer):\n",
        "    \"\"\"\n",
        "    Initializer for new AdditivePooling Layer.\n",
        "\n",
        "    Arguments:\n",
        "        num_classes -- The number of classes to be output'd\n",
        "    \"\"\"\n",
        "    ##################################################################################################################################\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 use_additive_pooling_processing=False,\n",
        "                 add_pool_process_max=128,\n",
        "                 **kwargs):\n",
        "        super(AdditivePooling, self).__init__(**kwargs)\n",
        "\n",
        "        self.num_classes=num_classes\n",
        "        \n",
        "        self.num_inputs=None\n",
        "        \n",
        "        self.use_additive_pooling_processing=use_additive_pooling_processing\n",
        "        \n",
        "        self.add_pool_process_max=add_pool_process_max\n",
        "    ##################################################################################################################################\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "\n",
        "        # The number of neurons must be collapsable into the number of classes.\n",
        "        # i.e if we have 10 classes, the number of neurons must be a divisor of 10.\n",
        "        assert input_shape[-1] % self.num_classes == 0\n",
        "\n",
        "        self.num_inputs = input_shape[-1]\n",
        "    ##################################################################################################################################\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.shape) >= 3:\n",
        "            output = tf.keras.backend.sum(inputs, axis=1)\n",
        "        else:\n",
        "            output = inputs\n",
        "\n",
        "        # Reshape the outputs:\n",
        "        output = tf.reshape(output, [-1, int(self.num_inputs/self.num_classes), self.num_classes])\n",
        "\n",
        "        # Sum up the neurons\n",
        "        output = tf.math.reduce_sum(output, 1)\n",
        "\n",
        "        if self.use_additive_pooling_processing:\n",
        "            # Scale the ouputs between 0 and add_pool_process_max:\n",
        "            max_val = tf.constant(self.add_pool_process_max, dtype=tf.float32)\n",
        "            max_output = tf.stack([tf.reduce_max(output,1) for i in range(self.num_classes)], axis=1)\n",
        "            max_output = tf.math.divide(max_val, max_output)\n",
        "            output = tf.math.multiply(output, max_output)\n",
        "\n",
        "            # Convert any NaN's to 0:\n",
        "            output = tf.where(tf.math.is_nan(output), tf.zeros_like(output), output)\n",
        "\n",
        "        return output\n",
        "    ##################################################################################################################################\n",
        "    def computer_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "\n",
        "        # Last dimension will be number of classes:\n",
        "        output_shape[-1] = self.num_classes\n",
        "\n",
        "        # Ticks were summed, so delete tick dimension if they exist:\n",
        "        if len(output_shape) >= 3:\n",
        "            del output_shape[1]\n",
        "\n",
        "        return tuple(output_shape)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5M8Utcf81kf"
      },
      "source": [
        "def opt_thresh_color(x_in):\n",
        "  \"\"\"\n",
        "  takes as input a numpy array of 2d color images, and applies optimal \n",
        "  color thresholding to them, transforming them to binary images\n",
        "  \"\"\"\n",
        "  x_bin = np.zeros_like(x_in[:,:,:,0])\n",
        "  x = np.copy(x_in)\n",
        "  for i in np.arange(len(x)):\n",
        "    for color in np.arange(3):\n",
        "      thresh = threshold_otsu(x[i,:,:,color])\n",
        "      x[i,:,:,color] = x[i,:,:,color] > thresh\n",
        "    x_bin[i] = np.logical_or(np.logical_or(np.logical_and(x[i,:,:,0],x[i,:,:,1]),\n",
        "                                           np.logical_and(x[i,:,:,1],x[i,:,:,2])),\n",
        "                             np.logical_and(x[i,:,:,0],x[i,:,:,2]))\n",
        "\n",
        "  return x_bin"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59zhBrzDB0dY"
      },
      "source": [
        "def opt_thresh_color_three(x_in):\n",
        "  \"\"\"\n",
        "  takes as input a numpy array of 2d color images, and applies optimal \n",
        "  color thresholding to them, transforming them to binary images\n",
        "  \"\"\"\n",
        "  #x_bin = np.zeros_like(x_in[:,:,:,0])\n",
        "  x = np.copy(x_in)\n",
        "  for i in np.arange(len(x)):\n",
        "    for color in np.arange(3):\n",
        "      thresh = threshold_otsu(x[i,:,:,color])\n",
        "      x[i,:,:,color] = x[i,:,:,color] > thresh\n",
        "    #x_bin[i] = np.logical_or(np.logical_or(np.logical_and(x[i,:,:,0],x[i,:,:,1]),\n",
        "    #                                       np.logical_and(x[i,:,:,1],x[i,:,:,2])),\n",
        "    #                         np.logical_and(x[i,:,:,0],x[i,:,:,2]))\n",
        "\n",
        "  return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9ZU_VMh2Bm7"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras import activations, losses\n",
        "from keras.layers import Conv2D, Flatten, Input, Activation, Dense, Dropout, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from skimage.filters import threshold_otsu\n",
        " \n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM1UT1Wc2JcS"
      },
      "source": [
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = opt_thresh_color_three(x_train)\n",
        "x_test = opt_thresh_color_three(x_test)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO0_3Kmq94-U",
        "outputId": "e54853f1-d557-4671-bb71-fa2595ed5268"
      },
      "source": [
        "  \n",
        "inputs = Input(shape=(32,32,3))\n",
        "\n",
        "# network = Conv2D(10, (7,7), name='1cwt', activation=activations.relu, kernel_regularizer='L2')(inputs)\n",
        "\n",
        "# maxpool = MaxPooling2D()(network)\n",
        "\n",
        "# network2 = Conv2D(15, (5,5), name='cwt', activation=activations.relu)(maxpool)\n",
        "\n",
        "# maxpool2 = MaxPooling2D()(network2)\n",
        "\n",
        "network3 = Conv2D(15, (11,11), name='2cwt', activation=activations.relu)(inputs)\n",
        "\n",
        "maxpool3 = MaxPooling2D()(network3)\n",
        "\n",
        "droppy = Dropout(0.25)(maxpool3)\n",
        "\n",
        "flattened = Flatten()(droppy)\n",
        "\n",
        "dense = Tea(units=120, name=\"tea_2\")(flattened)\n",
        "\n",
        "pooling = AdditivePooling(10)(dense)\n",
        "\n",
        "predictions = Activation('softmax')(pooling)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.reshape(-1, 32, 32, 3) \n",
        "x_test = x_test.reshape(-1, 32, 32, 3)\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=100, verbose=1, validation_split=0.2)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Test Loss: \", score[0]) \n",
        "print(\"Test Accuracy: \", score[1])\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - 5s 11ms/step - loss: 2.4367 - accuracy: 0.1678 - val_loss: 2.0618 - val_accuracy: 0.2339\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.0360 - accuracy: 0.2843 - val_loss: 1.9326 - val_accuracy: 0.2778\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.9131 - accuracy: 0.3207 - val_loss: 1.9252 - val_accuracy: 0.2842\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.8836 - accuracy: 0.3397 - val_loss: 1.8559 - val_accuracy: 0.3192\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.8312 - accuracy: 0.3542 - val_loss: 1.8296 - val_accuracy: 0.3421\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.8093 - accuracy: 0.3625 - val_loss: 1.8331 - val_accuracy: 0.3145\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.8033 - accuracy: 0.3741 - val_loss: 1.8259 - val_accuracy: 0.3389\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7651 - accuracy: 0.3866 - val_loss: 1.7917 - val_accuracy: 0.3359\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7638 - accuracy: 0.3846 - val_loss: 1.7751 - val_accuracy: 0.3641\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7486 - accuracy: 0.3913 - val_loss: 1.7736 - val_accuracy: 0.3486\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7247 - accuracy: 0.3991 - val_loss: 1.7403 - val_accuracy: 0.3655\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7142 - accuracy: 0.4008 - val_loss: 1.7527 - val_accuracy: 0.3822\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7020 - accuracy: 0.4100 - val_loss: 1.7353 - val_accuracy: 0.3835\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6888 - accuracy: 0.4167 - val_loss: 1.7108 - val_accuracy: 0.3771\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.6908 - accuracy: 0.4158 - val_loss: 1.7069 - val_accuracy: 0.3820\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6555 - accuracy: 0.4274 - val_loss: 1.6909 - val_accuracy: 0.3749\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6359 - accuracy: 0.4344 - val_loss: 1.6760 - val_accuracy: 0.3900\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6334 - accuracy: 0.4342 - val_loss: 1.7047 - val_accuracy: 0.3907\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6358 - accuracy: 0.4369 - val_loss: 1.6739 - val_accuracy: 0.4000\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6090 - accuracy: 0.4454 - val_loss: 1.6684 - val_accuracy: 0.3986\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6084 - accuracy: 0.4486 - val_loss: 1.6519 - val_accuracy: 0.3994\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5927 - accuracy: 0.4557 - val_loss: 1.6599 - val_accuracy: 0.3916\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.5694 - accuracy: 0.4577 - val_loss: 1.6403 - val_accuracy: 0.3975\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.5741 - accuracy: 0.4561 - val_loss: 1.6354 - val_accuracy: 0.4114\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.5597 - accuracy: 0.4662 - val_loss: 1.6366 - val_accuracy: 0.4160\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5445 - accuracy: 0.4699 - val_loss: 1.6341 - val_accuracy: 0.4074\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5589 - accuracy: 0.4646 - val_loss: 1.6219 - val_accuracy: 0.4014\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.5368 - accuracy: 0.4723 - val_loss: 1.6278 - val_accuracy: 0.4033\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5323 - accuracy: 0.4732 - val_loss: 1.6074 - val_accuracy: 0.4118\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5168 - accuracy: 0.4781 - val_loss: 1.6322 - val_accuracy: 0.4152\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5248 - accuracy: 0.4780 - val_loss: 1.6375 - val_accuracy: 0.4294\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5191 - accuracy: 0.4807 - val_loss: 1.6161 - val_accuracy: 0.4279\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5053 - accuracy: 0.4872 - val_loss: 1.6040 - val_accuracy: 0.4301\n",
            "Epoch 34/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5012 - accuracy: 0.4856 - val_loss: 1.5972 - val_accuracy: 0.4458\n",
            "Epoch 35/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.5011 - accuracy: 0.4870 - val_loss: 1.5869 - val_accuracy: 0.4378\n",
            "Epoch 36/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4785 - accuracy: 0.4966 - val_loss: 1.5757 - val_accuracy: 0.4356\n",
            "Epoch 37/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4830 - accuracy: 0.4942 - val_loss: 1.5946 - val_accuracy: 0.4354\n",
            "Epoch 38/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4673 - accuracy: 0.4977 - val_loss: 1.5764 - val_accuracy: 0.4357\n",
            "Epoch 39/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4700 - accuracy: 0.4985 - val_loss: 1.5846 - val_accuracy: 0.4301\n",
            "Epoch 40/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4612 - accuracy: 0.5008 - val_loss: 1.5736 - val_accuracy: 0.4416\n",
            "Epoch 41/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4674 - accuracy: 0.5007 - val_loss: 1.5713 - val_accuracy: 0.4389\n",
            "Epoch 42/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4580 - accuracy: 0.5054 - val_loss: 1.5694 - val_accuracy: 0.4406\n",
            "Epoch 43/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4558 - accuracy: 0.5024 - val_loss: 1.5734 - val_accuracy: 0.4322\n",
            "Epoch 44/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4414 - accuracy: 0.5102 - val_loss: 1.5855 - val_accuracy: 0.4408\n",
            "Epoch 45/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4384 - accuracy: 0.5104 - val_loss: 1.5814 - val_accuracy: 0.4243\n",
            "Epoch 46/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4422 - accuracy: 0.5087 - val_loss: 1.5550 - val_accuracy: 0.4376\n",
            "Epoch 47/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4450 - accuracy: 0.5092 - val_loss: 1.5770 - val_accuracy: 0.4344\n",
            "Epoch 48/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4344 - accuracy: 0.5134 - val_loss: 1.5501 - val_accuracy: 0.4509\n",
            "Epoch 49/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4274 - accuracy: 0.5191 - val_loss: 1.5506 - val_accuracy: 0.4524\n",
            "Epoch 50/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4167 - accuracy: 0.5199 - val_loss: 1.5497 - val_accuracy: 0.4506\n",
            "Epoch 51/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4112 - accuracy: 0.5206 - val_loss: 1.5676 - val_accuracy: 0.4473\n",
            "Epoch 52/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4163 - accuracy: 0.5139 - val_loss: 1.5617 - val_accuracy: 0.4297\n",
            "Epoch 53/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4122 - accuracy: 0.5197 - val_loss: 1.5679 - val_accuracy: 0.4396\n",
            "Epoch 54/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4074 - accuracy: 0.5233 - val_loss: 1.5643 - val_accuracy: 0.4348\n",
            "Epoch 55/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4207 - accuracy: 0.5148 - val_loss: 1.5640 - val_accuracy: 0.4543\n",
            "Epoch 56/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4008 - accuracy: 0.5257 - val_loss: 1.5535 - val_accuracy: 0.4459\n",
            "Epoch 57/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3895 - accuracy: 0.5260 - val_loss: 1.5491 - val_accuracy: 0.4434\n",
            "Epoch 58/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3942 - accuracy: 0.5221 - val_loss: 1.5385 - val_accuracy: 0.4523\n",
            "Epoch 59/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3801 - accuracy: 0.5334 - val_loss: 1.5561 - val_accuracy: 0.4418\n",
            "Epoch 60/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3909 - accuracy: 0.5266 - val_loss: 1.5262 - val_accuracy: 0.4527\n",
            "Epoch 61/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3771 - accuracy: 0.5322 - val_loss: 1.5345 - val_accuracy: 0.4490\n",
            "Epoch 62/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3768 - accuracy: 0.5286 - val_loss: 1.5537 - val_accuracy: 0.4407\n",
            "Epoch 63/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4011 - accuracy: 0.5251 - val_loss: 1.5414 - val_accuracy: 0.4438\n",
            "Epoch 64/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.3838 - accuracy: 0.5245 - val_loss: 1.5366 - val_accuracy: 0.4429\n",
            "Epoch 65/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3714 - accuracy: 0.5356 - val_loss: 1.5827 - val_accuracy: 0.4501\n",
            "Epoch 66/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.4005 - accuracy: 0.5241 - val_loss: 1.5235 - val_accuracy: 0.4562\n",
            "Epoch 67/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3703 - accuracy: 0.5320 - val_loss: 1.5430 - val_accuracy: 0.4493\n",
            "Epoch 68/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3852 - accuracy: 0.5303 - val_loss: 1.5272 - val_accuracy: 0.4547\n",
            "Epoch 69/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.3776 - accuracy: 0.5326 - val_loss: 1.5225 - val_accuracy: 0.4637\n",
            "Epoch 70/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3612 - accuracy: 0.5366 - val_loss: 1.5295 - val_accuracy: 0.4490\n",
            "Epoch 71/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3664 - accuracy: 0.5335 - val_loss: 1.5182 - val_accuracy: 0.4727\n",
            "Epoch 72/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.3577 - accuracy: 0.5383 - val_loss: 1.5142 - val_accuracy: 0.4593\n",
            "Epoch 73/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3631 - accuracy: 0.5360 - val_loss: 1.5252 - val_accuracy: 0.4574\n",
            "Epoch 74/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3542 - accuracy: 0.5404 - val_loss: 1.5306 - val_accuracy: 0.4571\n",
            "Epoch 75/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3667 - accuracy: 0.5387 - val_loss: 1.5231 - val_accuracy: 0.4594\n",
            "Epoch 76/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3400 - accuracy: 0.5465 - val_loss: 1.5296 - val_accuracy: 0.4581\n",
            "Epoch 77/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3484 - accuracy: 0.5444 - val_loss: 1.5269 - val_accuracy: 0.4606\n",
            "Epoch 78/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3472 - accuracy: 0.5440 - val_loss: 1.5127 - val_accuracy: 0.4621\n",
            "Epoch 79/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3424 - accuracy: 0.5458 - val_loss: 1.5254 - val_accuracy: 0.4564\n",
            "Epoch 80/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3388 - accuracy: 0.5477 - val_loss: 1.5321 - val_accuracy: 0.4584\n",
            "Epoch 81/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3481 - accuracy: 0.5449 - val_loss: 1.5361 - val_accuracy: 0.4533\n",
            "Epoch 82/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3386 - accuracy: 0.5446 - val_loss: 1.5221 - val_accuracy: 0.4570\n",
            "Epoch 83/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3189 - accuracy: 0.5531 - val_loss: 1.5296 - val_accuracy: 0.4577\n",
            "Epoch 84/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3425 - accuracy: 0.5430 - val_loss: 1.5261 - val_accuracy: 0.4630\n",
            "Epoch 85/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3370 - accuracy: 0.5467 - val_loss: 1.5259 - val_accuracy: 0.4623\n",
            "Epoch 86/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3362 - accuracy: 0.5481 - val_loss: 1.5207 - val_accuracy: 0.4667\n",
            "Epoch 87/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3363 - accuracy: 0.5473 - val_loss: 1.5416 - val_accuracy: 0.4598\n",
            "Epoch 88/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3285 - accuracy: 0.5521 - val_loss: 1.5182 - val_accuracy: 0.4660\n",
            "Epoch 89/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3215 - accuracy: 0.5524 - val_loss: 1.5357 - val_accuracy: 0.4552\n",
            "Epoch 90/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3227 - accuracy: 0.5546 - val_loss: 1.5329 - val_accuracy: 0.4704\n",
            "Epoch 91/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3355 - accuracy: 0.5478 - val_loss: 1.5149 - val_accuracy: 0.4694\n",
            "Epoch 92/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3376 - accuracy: 0.5482 - val_loss: 1.5058 - val_accuracy: 0.4662\n",
            "Epoch 93/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3230 - accuracy: 0.5561 - val_loss: 1.5114 - val_accuracy: 0.4655\n",
            "Epoch 94/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.3119 - accuracy: 0.5540 - val_loss: 1.5254 - val_accuracy: 0.4619\n",
            "Epoch 95/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3232 - accuracy: 0.5531 - val_loss: 1.5299 - val_accuracy: 0.4559\n",
            "Epoch 96/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3250 - accuracy: 0.5510 - val_loss: 1.5067 - val_accuracy: 0.4685\n",
            "Epoch 97/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3174 - accuracy: 0.5512 - val_loss: 1.5227 - val_accuracy: 0.4688\n",
            "Epoch 98/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3144 - accuracy: 0.5559 - val_loss: 1.5116 - val_accuracy: 0.4697\n",
            "Epoch 99/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3153 - accuracy: 0.5524 - val_loss: 1.5049 - val_accuracy: 0.4703\n",
            "Epoch 100/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3055 - accuracy: 0.5550 - val_loss: 1.5039 - val_accuracy: 0.4643\n",
            "Test Loss:  1.5061702728271484\n",
            "Test Accuracy:  0.4700999855995178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptnv7td5ORir",
        "outputId": "1894fb1e-583f-44e7-b417-fead7d1d4f84"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "2cwt (Conv2D)                (None, 22, 22, 15)        5460      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 11, 11, 15)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 11, 11, 15)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1815)              0         \n",
            "_________________________________________________________________\n",
            "tea_2 (Tea)                  (None, 120)               435720    \n",
            "_________________________________________________________________\n",
            "additive_pooling (AdditivePo (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 441,180\n",
            "Trainable params: 223,380\n",
            "Non-trainable params: 217,800\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luJtzwzeOire",
        "outputId": "01857736-425b-4c22-cc56-280441ef8594"
      },
      "source": [
        "orig_weights, constrained_weights = constrain_weights(model)\n",
        "model.set_weights(constrained_weights)\n",
        "\n",
        "print_weights(model)\n",
        "\n",
        "print(\"Post-Ternary Constraint Accuracy: \")\n",
        "\n",
        "# Evaluate the constained weight model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "test_predictions = model.predict(x_test)\n",
        "\n",
        "print(\"Test Loss: \", score[0])\n",
        "print(\"Test Accuracy: \", score[1])\n",
        "\n",
        "# Restore the original weights temporarily so we can evaluate them\n",
        "model.set_weights(orig_weights)\n",
        "\n",
        "print(\"Original Floating-Point Accuracy: \")\n",
        "\n",
        "# Evaluate the original, floating-point weight model\n",
        "float_score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Test Loss: \", float_score[0])\n",
        "print(\"Test Accuracy: \", float_score[1])\n",
        "\n",
        "print(\"Accuracy loss due to train-then-constrain: \" ,float_score[1] - score[1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2cwt/kernel:0 weights: \n",
            "(11, 11, 3, 15)\n",
            "0.0-1.00.0 0.00.00.0 0.00.00.0 -1.0-1.00.0 -1.00.00.0 0.00.00.0 0.0-1.0-1.0 0.0-1.00.0 0.00.0-1.0 0.00.0-1.0 0.00.0-1.0 \n",
            "0.00.00.0 1.0-1.00.0 -1.0-1.01.0 0.00.00.0 -1.0-1.00.0 0.00.00.0 0.00.0-1.0 -1.0-1.0-1.0 0.00.00.0 0.00.0-1.0 1.01.0-1.0 \n",
            "0.00.00.0 -1.0-1.00.0 -1.0-1.00.0 -1.00.00.0 -1.00.00.0 -1.00.00.0 0.0-1.0-1.0 -1.0-1.0-1.0 0.0-1.0-1.0 1.00.0-1.0 0.01.00.0 \n",
            "-1.0-1.00.0 -1.0-1.00.0 0.0-1.00.0 0.0-1.0-1.0 -1.0-1.00.0 0.00.00.0 0.0-1.0-1.0 -1.0-1.0-1.0 0.00.00.0 1.01.00.0 1.00.00.0 \n",
            "-1.0-1.0-1.0 -1.00.01.0 -1.0-1.00.0 1.00.0-1.0 0.00.00.0 -1.0-1.0-1.0 -1.0-1.0-1.0 0.0-1.00.0 1.00.00.0 1.00.01.0 1.00.00.0 \n",
            "-1.0-1.00.0 -1.0-1.00.0 -1.0-1.00.0 -1.0-1.00.0 0.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 0.00.00.0 0.00.00.0 1.01.00.0 1.00.00.0 \n",
            "-1.0-1.00.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.00.0-1.0 1.00.00.0 0.00.01.0 0.01.01.0 1.00.00.0 1.00.00.0 \n",
            "-1.00.01.0 0.00.00.0 0.0-1.00.0 -1.0-1.0-1.0 -1.0-1.0-1.0 0.00.0-1.0 1.00.0-1.0 0.00.00.0 0.00.00.0 1.00.00.0 1.00.00.0 \n",
            "-1.0-1.01.0 -1.00.0-1.0 -1.0-1.01.0 -1.0-1.00.0 0.00.00.0 0.0-1.00.0 1.00.0-1.0 1.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "-1.00.00.0 -1.00.01.0 -1.00.00.0 0.00.00.0 1.00.00.0 1.00.00.0 1.00.00.0 1.01.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 1.00.00.0 0.01.00.0 0.01.00.0 0.00.01.0 0.00.00.0 0.00.00.0 0.0-1.00.0 \n",
            "Next Filter: \n",
            "0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.00.01.0 0.00.01.0 0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.01.00.0 0.01.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 -1.00.00.0 -1.00.00.0 -1.00.00.0 0.00.00.0 0.00.01.0 0.00.00.0 -1.00.00.0 -1.00.00.0 -1.00.00.0 -1.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 -1.00.00.0 0.00.0-1.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.00.01.0 0.00.00.0 0.00.01.0 0.0-1.00.0 0.0-1.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.01.0 0.00.01.0 0.00.01.0 0.0-1.00.0 0.0-1.0-1.0 0.0-1.0-1.0 0.0-1.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "1.00.01.0 1.00.01.0 1.00.01.0 0.0-1.00.0 -1.0-1.00.0 0.0-1.00.0 0.0-1.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "1.00.01.0 1.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "-1.0-1.00.0 0.0-1.00.0 0.00.00.0 0.00.00.0 0.01.00.0 0.01.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.0-1.0 \n",
            "-1.0-1.0-1.0 -1.0-1.0-1.0 0.00.00.0 1.01.00.0 1.01.01.0 0.00.01.0 0.00.00.0 0.00.00.0 0.01.00.0 0.00.00.0 0.00.00.0 \n",
            "-1.0-1.00.0 -1.0-1.00.0 -1.0-1.00.0 0.0-1.00.0 0.00.00.0 0.00.00.0 1.01.00.0 0.01.00.0 0.00.0-1.0 0.00.0-1.0 -1.00.0-1.0 \n",
            "Next Filter: \n",
            "0.00.00.0 1.00.00.0 1.0-1.00.0 1.0-1.00.0 0.0-1.00.0 0.00.00.0 1.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 -1.00.00.0 \n",
            "0.00.00.0 1.00.01.0 0.0-1.00.0 0.0-1.0-1.0 0.00.00.0 0.00.01.0 1.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 -1.00.00.0 \n",
            "0.00.00.0 1.00.01.0 0.0-1.0-1.0 -1.0-1.0-1.0 0.00.00.0 1.01.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 -1.00.00.0 \n",
            "1.00.01.0 1.01.01.0 -1.0-1.0-1.0 -1.0-1.0-1.0 1.01.01.0 1.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 -1.00.00.0 \n",
            "1.00.01.0 1.01.01.0 -1.0-1.0-1.0 -1.0-1.0-1.0 1.01.01.0 1.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 -1.00.00.0 -1.00.00.0 \n",
            "1.00.01.0 1.01.01.0 -1.0-1.0-1.0 -1.0-1.0-1.0 1.01.01.0 0.00.01.0 0.0-1.0-1.0 0.00.00.0 0.01.00.0 0.00.00.0 0.00.00.0 \n",
            "1.00.01.0 1.01.01.0 -1.0-1.0-1.0 -1.00.0-1.0 1.01.01.0 0.00.00.0 -1.0-1.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "1.00.00.0 0.00.00.0 -1.0-1.0-1.0 0.00.00.0 1.01.01.0 0.00.00.0 0.0-1.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 -1.00.00.0 \n",
            "1.00.00.0 0.00.00.0 -1.0-1.0-1.0 0.00.00.0 0.01.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 -1.00.00.0 \n",
            "0.00.00.0 0.00.00.0 -1.00.0-1.0 0.01.00.0 0.00.01.0 0.00.00.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.0-1.0 \n",
            "0.00.00.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.01.00.0 0.00.00.0 0.00.00.0 \n",
            "Next Filter: \n",
            "-1.00.0-1.0 -1.00.0-1.0 1.00.00.0 0.0-1.00.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.0-1.0 0.00.00.0 -1.00.00.0 \n",
            "-1.00.0-1.0 -1.00.00.0 1.01.00.0 0.00.00.0 0.00.00.0 0.00.01.0 0.00.00.0 1.00.00.0 0.00.0-1.0 0.00.0-1.0 0.00.00.0 \n",
            "1.01.0-1.0 0.00.0-1.0 0.00.00.0 -1.00.00.0 0.00.0-1.0 -1.01.00.0 0.01.00.0 0.00.01.0 0.00.00.0 0.00.00.0 0.01.00.0 \n",
            "0.01.0-1.0 0.00.00.0 0.00.00.0 0.01.01.0 -1.00.00.0 0.01.00.0 0.01.01.0 0.01.00.0 -1.01.0-1.0 0.01.00.0 0.00.00.0 \n",
            "0.00.0-1.0 -1.00.0-1.0 0.00.00.0 0.00.00.0 -1.01.00.0 0.01.0-1.0 0.01.00.0 0.01.0-1.0 0.00.0-1.0 0.01.0-1.0 0.01.0-1.0 \n",
            "0.00.0-1.0 -1.00.0-1.0 0.01.00.0 0.00.00.0 -1.01.0-1.0 0.01.0-1.0 0.01.00.0 0.01.0-1.0 0.01.0-1.0 0.01.00.0 -1.00.00.0 \n",
            "0.01.0-1.0 0.00.0-1.0 0.01.00.0 0.01.01.0 -1.01.0-1.0 -1.01.0-1.0 -1.01.00.0 0.01.0-1.0 -1.01.0-1.0 0.00.00.0 -1.00.00.0 \n",
            "0.00.00.0 -1.00.0-1.0 0.01.00.0 0.01.01.0 0.01.0-1.0 -1.01.0-1.0 0.01.0-1.0 -1.01.0-1.0 0.01.00.0 0.00.00.0 -1.0-1.00.0 \n",
            "0.0-1.01.0 -1.00.0-1.0 0.0-1.00.0 1.00.00.0 0.01.00.0 -1.01.0-1.0 -1.01.0-1.0 1.01.00.0 0.00.00.0 0.0-1.00.0 -1.0-1.00.0 \n",
            "0.0-1.00.0 0.00.00.0 -1.00.00.0 0.00.00.0 0.00.0-1.0 0.00.0-1.0 0.00.0-1.0 0.00.0-1.0 -1.00.0-1.0 -1.00.00.0 -1.0-1.0-1.0 \n",
            "0.0-1.01.0 0.0-1.01.0 0.00.01.0 0.00.01.0 0.01.01.0 0.00.00.0 0.00.01.0 0.01.00.0 0.0-1.00.0 0.00.01.0 0.0-1.01.0 \n",
            "Next Filter: \n",
            "-1.0-1.01.0 0.0-1.00.0 0.00.00.0 1.00.0-1.0 0.00.0-1.0 0.00.00.0 1.00.00.0 0.0-1.0-1.0 0.01.0-1.0 1.01.0-1.0 0.00.0-1.0 \n",
            "0.00.01.0 0.0-1.00.0 0.00.01.0 0.01.00.0 -1.0-1.0-1.0 0.01.00.0 1.01.01.0 0.00.0-1.0 0.00.00.0 1.01.0-1.0 0.00.0-1.0 \n",
            "0.00.00.0 -1.0-1.00.0 1.00.01.0 1.01.01.0 -1.0-1.0-1.0 0.00.00.0 1.01.01.0 0.00.0-1.0 0.00.0-1.0 0.00.00.0 0.00.0-1.0 \n",
            "0.00.01.0 -1.0-1.00.0 1.00.01.0 1.01.01.0 -1.0-1.0-1.0 0.00.0-1.0 0.01.01.0 0.00.01.0 0.00.00.0 0.01.0-1.0 0.00.00.0 \n",
            "0.00.00.0 -1.00.01.0 1.00.00.0 1.01.01.0 -1.0-1.0-1.0 -1.00.0-1.0 0.00.00.0 1.00.00.0 0.01.00.0 0.00.0-1.0 0.00.00.0 \n",
            "-1.00.00.0 -1.0-1.0-1.0 0.00.01.0 1.01.01.0 -1.0-1.0-1.0 0.00.00.0 -1.00.00.0 1.00.00.0 0.00.00.0 0.00.0-1.0 0.00.00.0 \n",
            "-1.00.01.0 -1.0-1.0-1.0 1.00.00.0 1.01.01.0 0.00.00.0 -1.00.0-1.0 0.00.0-1.0 0.00.01.0 0.00.00.0 0.00.00.0 0.0-1.00.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.01.00.0 -1.0-1.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.01.00.0 0.00.00.0 \n",
            "0.0-1.00.0 0.00.00.0 0.0-1.00.0 0.00.0-1.0 1.01.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.0-1.0 \n",
            "0.0-1.01.0 1.00.00.0 0.00.00.0 0.0-1.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.0-1.0 \n",
            "0.00.00.0 0.0-1.00.0 1.00.01.0 0.00.00.0 0.00.00.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.01.00.0 0.00.00.0 1.00.00.0 \n",
            "Next Filter: \n",
            "0.00.0-1.0 -1.00.0-1.0 0.00.00.0 0.00.00.0 1.01.00.0 -1.00.00.0 0.00.00.0 -1.0-1.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.0-1.0 0.00.00.0 0.01.00.0 1.00.00.0 0.00.00.0 0.01.01.0 0.00.00.0 0.0-1.0-1.0 0.0-1.0-1.0 0.00.00.0 1.00.01.0 \n",
            "0.00.00.0 0.00.00.0 1.01.01.0 -1.0-1.0-1.0 0.00.00.0 1.01.01.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.01.0 \n",
            "-1.00.0-1.0 1.00.00.0 0.00.00.0 -1.00.0-1.0 0.00.00.0 -1.00.0-1.0 -1.00.0-1.0 0.00.00.0 0.00.00.0 0.00.01.0 0.0-1.00.0 \n",
            "0.00.00.0 0.00.00.0 -1.0-1.00.0 -1.00.00.0 0.01.00.0 -1.00.0-1.0 -1.00.0-1.0 0.00.00.0 1.01.01.0 0.00.01.0 -1.0-1.0-1.0 \n",
            "1.00.00.0 -1.00.00.0 0.00.00.0 1.01.01.0 -1.00.0-1.0 -1.00.0-1.0 -1.00.0-1.0 1.01.01.0 1.01.01.0 0.00.01.0 -1.0-1.0-1.0 \n",
            "0.0-1.0-1.0 0.00.00.0 -1.00.00.0 0.01.00.0 0.00.0-1.0 0.00.0-1.0 1.01.01.0 1.01.01.0 0.00.00.0 -1.0-1.0-1.0 0.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.00.0-1.0 1.01.0-1.0 1.01.01.0 1.01.01.0 0.00.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 1.01.01.0 \n",
            "0.00.00.0 0.00.00.0 0.01.00.0 0.00.0-1.0 1.00.01.0 1.01.01.0 0.00.00.0 -1.0-1.0-1.0 -1.0-1.0-1.0 1.01.01.0 0.00.01.0 \n",
            "0.01.01.0 0.0-1.0-1.0 0.00.00.0 0.01.01.0 -1.00.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 0.00.0-1.0 1.01.01.0 1.01.01.0 0.0-1.00.0 \n",
            "0.00.00.0 0.01.01.0 0.00.0-1.0 0.0-1.00.0 0.0-1.00.0 -1.0-1.0-1.0 -1.0-1.0-1.0 1.01.01.0 1.01.01.0 0.0-1.0-1.0 0.0-1.00.0 \n",
            "Next Filter: \n",
            "0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 1.01.00.0 1.00.00.0 0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.0-1.00.0 0.00.00.0 0.00.0-1.0 0.00.0-1.0 0.0-1.0-1.0 0.00.0-1.0 0.00.00.0 1.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.0-1.0 0.01.00.0 0.01.00.0 0.00.00.0 0.00.00.0 0.00.0-1.0 \n",
            "0.00.01.0 0.00.01.0 -1.00.00.0 0.00.0-1.0 -1.0-1.0-1.0 0.00.0-1.0 0.00.0-1.0 0.00.00.0 0.00.00.0 1.01.00.0 1.01.00.0 \n",
            "-1.00.01.0 -1.00.00.0 0.00.01.0 -1.00.00.0 -1.0-1.00.0 -1.0-1.0-1.0 0.0-1.0-1.0 0.0-1.0-1.0 0.00.0-1.0 0.00.00.0 0.00.00.0 \n",
            "-1.00.01.0 -1.00.00.0 0.00.01.0 -1.00.01.0 -1.00.01.0 -1.00.00.0 -1.00.00.0 -1.00.00.0 -1.00.0-1.0 0.00.00.0 0.00.00.0 \n",
            "0.00.01.0 0.01.01.0 0.00.01.0 0.01.01.0 0.01.01.0 -1.00.01.0 -1.00.01.0 -1.00.00.0 -1.0-1.00.0 0.00.01.0 0.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.00.01.0 0.00.01.0 0.00.01.0 -1.00.01.0 0.00.01.0 0.00.01.0 -1.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.00.01.0 -1.00.00.0 -1.00.00.0 0.00.01.0 0.00.01.0 -1.00.01.0 -1.00.00.0 0.00.01.0 0.00.01.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.01.00.0 0.00.01.0 0.00.00.0 1.00.01.0 0.00.01.0 0.00.01.0 0.00.01.0 0.00.00.0 \n",
            "-1.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.0-1.0 0.00.0-1.0 -1.0-1.00.0 \n",
            "Next Filter: \n",
            "0.00.00.0 0.00.00.0 0.00.01.0 0.00.00.0 0.00.0-1.0 0.0-1.0-1.0 0.00.00.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.01.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.00.0-1.0 0.00.0-1.0 0.00.00.0 1.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.01.0 0.00.01.0 \n",
            "0.00.00.0 0.00.00.0 1.00.0-1.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.01.0 \n",
            "0.00.0-1.0 1.00.0-1.0 0.00.0-1.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.01.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.01.0 \n",
            "0.0-1.0-1.0 0.00.0-1.0 0.00.0-1.0 0.0-1.0-1.0 0.00.00.0 0.00.01.0 0.00.01.0 0.00.01.0 0.00.01.0 0.01.01.0 0.01.01.0 \n",
            "0.00.0-1.0 0.00.0-1.0 0.00.0-1.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 -1.00.01.0 0.00.01.0 0.00.01.0 0.01.01.0 \n",
            "0.00.0-1.0 0.00.0-1.0 0.00.0-1.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.01.0 0.00.01.0 0.00.01.0 0.00.00.0 \n",
            "0.0-1.0-1.0 0.00.0-1.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.01.0 0.00.01.0 \n",
            "0.00.0-1.0 0.00.0-1.0 1.00.00.0 1.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.0-1.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.0-1.0-1.0 0.00.00.0 1.00.00.0 0.00.00.0 0.00.00.0 0.0-1.00.0 0.00.00.0 0.00.0-1.0 -1.00.00.0 0.00.00.0 0.00.00.0 \n",
            "Next Filter: \n",
            "0.00.00.0 0.0-1.00.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.0-1.0-1.0 0.0-1.0-1.0 0.00.0-1.0 0.00.0-1.0 0.00.00.0 -1.01.00.0 \n",
            "0.00.00.0 0.00.0-1.0 0.0-1.0-1.0 0.00.0-1.0 0.00.0-1.0 0.00.0-1.0 0.00.00.0 0.01.00.0 0.01.01.0 0.01.01.0 -1.00.00.0 \n",
            "0.0-1.0-1.0 0.00.0-1.0 0.00.0-1.0 0.00.00.0 1.01.00.0 1.01.01.0 0.01.01.0 0.01.01.0 -1.00.01.0 -1.00.00.0 -1.0-1.00.0 \n",
            "1.00.00.0 1.01.00.0 1.01.01.0 1.01.01.0 1.01.01.0 0.01.01.0 -1.00.01.0 -1.0-1.00.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 \n",
            "0.00.0-1.0 0.00.00.0 0.01.00.0 0.01.01.0 0.01.01.0 -1.00.01.0 -1.0-1.00.0 -1.0-1.0-1.0 -1.0-1.0-1.0 0.00.00.0 -1.00.00.0 \n",
            "1.01.00.0 1.01.01.0 0.01.01.0 -1.00.01.0 -1.0-1.00.0 -1.0-1.0-1.0 -1.0-1.0-1.0 0.00.01.0 -1.00.00.0 0.00.0-1.0 -1.0-1.00.0 \n",
            "1.00.00.0 0.01.01.0 0.00.01.0 0.00.01.0 -1.0-1.00.0 0.0-1.00.0 0.01.01.0 0.00.01.0 -1.0-1.00.0 0.00.00.0 -1.00.00.0 \n",
            "1.00.00.0 0.00.00.0 0.00.01.0 0.01.01.0 0.00.00.0 0.00.00.0 0.00.01.0 0.0-1.00.0 0.00.00.0 0.0-1.00.0 0.0-1.00.0 \n",
            "1.00.01.0 0.00.01.0 0.00.01.0 0.0-1.00.0 0.0-1.00.0 0.0-1.0-1.0 -1.0-1.0-1.0 0.0-1.0-1.0 0.0-1.0-1.0 0.01.0-1.0 1.00.00.0 \n",
            "0.0-1.00.0 0.0-1.00.0 0.0-1.00.0 0.00.00.0 0.0-1.00.0 -1.0-1.0-1.0 1.0-1.0-1.0 0.0-1.0-1.0 0.00.00.0 0.00.00.0 0.01.00.0 \n",
            "1.00.00.0 0.0-1.00.0 0.0-1.00.0 0.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.00.0 -1.0-1.00.0 0.00.00.0 1.00.01.0 0.00.01.0 -1.01.00.0 \n",
            "Next Filter: \n",
            "0.00.01.0 0.00.01.0 0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.0-1.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.01.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 1.00.00.0 0.01.00.0 0.00.00.0 0.00.00.0 1.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.01.00.0 0.00.00.0 0.0-1.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 1.00.00.0 1.01.01.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.0-1.0-1.0 0.0-1.0-1.0 0.00.00.0 0.00.00.0 0.0-1.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "-1.00.00.0 0.00.00.0 0.00.00.0 0.0-1.0-1.0 0.00.00.0 0.00.00.0 0.0-1.0-1.0 0.00.00.0 0.00.00.0 0.0-1.0-1.0 0.00.00.0 \n",
            "-1.0-1.00.0 0.01.01.0 1.00.01.0 0.00.01.0 1.01.01.0 1.01.01.0 0.00.00.0 1.00.00.0 1.01.01.0 0.00.00.0 0.01.00.0 \n",
            "-1.00.00.0 0.00.00.0 -1.0-1.00.0 -1.0-1.0-1.0 0.00.00.0 0.01.00.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.0-1.00.0 0.00.00.0 \n",
            "-1.0-1.00.0 0.00.00.0 0.00.00.0 0.00.00.0 1.01.01.0 0.01.00.0 0.00.00.0 1.01.01.0 0.00.00.0 0.00.0-1.0 0.00.00.0 \n",
            "0.00.00.0 0.01.01.0 0.01.01.0 1.00.00.0 1.01.01.0 1.01.01.0 1.01.01.0 1.01.00.0 0.01.00.0 0.00.00.0 -1.00.00.0 \n",
            "-1.00.0-1.0 -1.0-1.0-1.0 -1.0-1.00.0 -1.0-1.0-1.0 0.0-1.0-1.0 0.0-1.0-1.0 0.0-1.0-1.0 0.00.00.0 0.00.00.0 -1.00.00.0 0.01.00.0 \n",
            "0.01.01.0 -1.00.00.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.00.0-1.0 0.01.00.0 \n",
            "Next Filter: \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 1.00.00.0 0.00.00.0 1.00.00.0 1.00.00.0 0.00.00.0 \n",
            "0.00.00.0 -1.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 1.00.00.0 0.00.00.0 0.00.00.0 0.0-1.00.0 \n",
            "0.00.00.0 1.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 1.00.00.0 0.00.00.0 0.00.00.0 0.0-1.00.0 0.0-1.0-1.0 0.00.00.0 \n",
            "0.0-1.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 1.00.00.0 0.00.00.0 1.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.0-1.00.0 1.0-1.00.0 1.00.00.0 0.00.00.0 1.00.00.0 1.00.00.0 0.00.00.0 0.00.00.0 -1.0-1.0-1.0 0.00.00.0 \n",
            "0.00.00.0 0.00.00.0 1.01.01.0 1.01.01.0 1.01.00.0 1.00.01.0 1.01.00.0 0.00.0-1.0 0.00.00.0 -1.0-1.0-1.0 -1.01.00.0 \n",
            "-1.0-1.00.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.00.0-1.0 0.01.01.0 \n",
            "0.00.00.0 -1.0-1.00.0 0.0-1.00.0 -1.0-1.00.0 -1.0-1.00.0 -1.0-1.00.0 -1.00.00.0 -1.01.01.0 0.01.01.0 0.01.01.0 0.01.01.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 -1.00.00.0 0.00.01.0 -1.00.01.0 0.00.01.0 -1.00.00.0 0.00.00.0 0.01.00.0 \n",
            "0.00.00.0 0.00.00.0 0.01.00.0 0.01.01.0 0.01.01.0 0.01.01.0 0.01.00.0 0.01.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.01.00.0 0.01.00.0 0.01.00.0 0.01.00.0 0.01.00.0 0.01.00.0 0.00.00.0 0.00.00.0 \n",
            "Next Filter: \n",
            "-1.0-1.0-1.0 -1.0-1.0-1.0 0.01.00.0 1.01.01.0 0.01.00.0 -1.00.00.0 0.00.00.0 0.0-1.00.0 0.00.01.0 -1.00.0-1.0 -1.00.0-1.0 \n",
            "0.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.00.0 -1.0-1.00.0 0.00.01.0 1.00.01.0 0.0-1.0-1.0 0.0-1.00.0 0.0-1.01.0 0.00.0-1.0 -1.00.0-1.0 \n",
            "0.01.00.0 -1.00.0-1.0 -1.0-1.0-1.0 -1.00.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.01.0 0.00.0-1.0 0.00.00.0 1.00.01.0 \n",
            "0.01.01.0 0.00.00.0 -1.00.0-1.0 0.0-1.0-1.0 0.00.0-1.0 0.00.00.0 0.00.01.0 0.00.00.0 0.00.00.0 1.00.00.0 0.00.01.0 \n",
            "0.00.0-1.0 0.01.00.0 0.00.00.0 0.00.00.0 0.0-1.0-1.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.01.00.0 0.01.00.0 0.00.00.0 0.0-1.00.0 0.00.00.0 0.0-1.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.01.00.0 0.00.00.0 0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.0-1.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.01.0 0.00.00.0 0.00.00.0 0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.01.00.0 0.01.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.01.0 0.00.00.0 0.00.00.0 0.00.0-1.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.01.00.0 0.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.01.00.0 1.00.00.0 1.01.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 1.00.00.0 0.00.00.0 1.00.00.0 1.00.00.0 \n",
            "Next Filter: \n",
            "0.00.00.0 0.00.0-1.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.0-1.0 0.00.0-1.0 1.01.01.0 1.01.01.0 0.00.00.0 -1.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.00.0-1.0 0.00.00.0 1.00.00.0 0.00.0-1.0 -1.00.0-1.0 0.0-1.0-1.0 1.01.01.0 1.01.01.0 -1.00.00.0 \n",
            "0.01.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 1.00.00.0 0.00.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 1.01.01.0 1.01.01.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.00.0-1.0 0.00.00.0 0.00.01.0 1.01.01.0 0.00.0-1.0 -1.0-1.0-1.0 0.00.0-1.0 1.01.01.0 \n",
            "0.01.00.0 0.00.00.0 0.0-1.00.0 -1.00.00.0 1.01.00.0 0.00.00.0 -1.00.00.0 1.01.01.0 -1.0-1.0-1.0 -1.0-1.0-1.0 0.01.00.0 \n",
            "0.01.01.0 0.00.00.0 0.00.00.0 1.00.00.0 0.00.00.0 1.00.01.0 0.00.00.0 0.00.00.0 1.01.01.0 -1.0-1.0-1.0 -1.00.0-1.0 \n",
            "0.00.00.0 0.00.00.0 1.00.00.0 0.00.00.0 0.00.00.0 0.0-1.00.0 0.00.00.0 0.00.00.0 1.01.01.0 0.00.00.0 -1.0-1.0-1.0 \n",
            "0.00.00.0 0.00.00.0 1.00.00.0 0.00.00.0 -1.0-1.00.0 -1.0-1.00.0 1.00.01.0 1.01.00.0 0.00.00.0 0.01.00.0 -1.00.0-1.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.01.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 1.00.0-1.0 -1.00.0-1.0 \n",
            "0.00.00.0 -1.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.01.00.0 0.00.00.0 0.00.01.0 0.00.00.0 0.00.0-1.0 -1.0-1.0-1.0 \n",
            "0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.01.0 1.00.00.0 -1.0-1.0-1.0 0.00.00.0 0.00.01.0 1.00.01.0 -1.00.0-1.0 \n",
            "Next Filter: \n",
            "0.00.00.0 0.01.00.0 1.01.01.0 0.00.00.0 1.01.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 1.00.01.0 0.00.00.0 \n",
            "-1.00.0-1.0 0.00.00.0 0.00.00.0 -1.0-1.0-1.0 0.00.00.0 0.0-1.0-1.0 0.00.0-1.0 0.00.00.0 -1.00.00.0 0.00.00.0 0.00.00.0 \n",
            "0.00.00.0 0.01.00.0 1.01.01.0 -1.00.00.0 0.01.01.0 0.00.00.0 -1.00.00.0 0.00.00.0 0.0-1.0-1.0 0.01.00.0 0.00.00.0 \n",
            "-1.00.00.0 0.01.00.0 0.01.01.0 -1.00.0-1.0 0.01.01.0 0.00.00.0 -1.00.00.0 0.01.00.0 -1.00.00.0 0.01.01.0 -1.00.00.0 \n",
            "-1.0-1.0-1.0 0.01.00.0 1.01.01.0 -1.00.00.0 0.01.01.0 0.01.01.0 0.01.01.0 1.01.01.0 -1.00.00.0 0.01.01.0 -1.00.00.0 \n",
            "-1.0-1.0-1.0 -1.0-1.0-1.0 -1.0-1.00.0 -1.0-1.0-1.0 -1.0-1.00.0 -1.0-1.0-1.0 -1.0-1.0-1.0 -1.00.01.0 -1.0-1.00.0 0.01.01.0 -1.00.01.0 \n",
            "0.00.00.0 0.01.01.0 0.01.01.0 -1.0-1.0-1.0 0.00.01.0 0.00.00.0 -1.0-1.0-1.0 0.0-1.00.0 -1.0-1.0-1.0 -1.00.00.0 -1.0-1.0-1.0 \n",
            "1.00.00.0 1.00.00.0 1.01.01.0 0.00.00.0 1.01.01.0 1.01.00.0 1.00.00.0 1.01.01.0 1.00.00.0 1.01.01.0 0.00.00.0 \n",
            "1.00.00.0 1.01.00.0 1.01.00.0 0.0-1.00.0 1.00.00.0 1.00.00.0 1.00.0-1.0 1.00.00.0 0.0-1.00.0 1.00.00.0 1.00.00.0 \n",
            "0.0-1.0-1.0 0.00.0-1.0 0.0-1.0-1.0 -1.0-1.0-1.0 0.0-1.00.0 0.0-1.00.0 0.0-1.0-1.0 0.0-1.00.0 0.0-1.0-1.0 0.00.00.0 0.0-1.0-1.0 \n",
            "0.00.0-1.0 1.00.00.0 0.00.00.0 0.00.00.0 1.00.00.0 1.01.00.0 0.00.00.0 1.00.00.0 0.00.0-1.0 1.00.00.0 0.00.0-1.0 \n",
            "Next Filter: \n",
            "0.00.00.0 -1.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.0-1.0 0.00.00.0 1.00.0-1.0 1.00.00.0 1.00.00.0 1.01.00.0 \n",
            "-1.00.00.0 0.01.01.0 0.00.00.0 0.00.00.0 0.00.0-1.0 -1.00.0-1.0 0.0-1.0-1.0 1.0-1.0-1.0 1.00.0-1.0 1.00.00.0 1.00.00.0 \n",
            "-1.00.00.0 0.01.01.0 0.01.01.0 0.00.00.0 0.00.0-1.0 -1.00.0-1.0 -1.0-1.0-1.0 0.0-1.0-1.0 0.0-1.0-1.0 1.01.00.0 1.00.00.0 \n",
            "0.00.00.0 0.00.01.0 0.00.00.0 0.01.00.0 0.01.00.0 0.00.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 0.0-1.0-1.0 1.00.00.0 1.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.01.00.0 0.00.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 1.0-1.0-1.0 1.00.00.0 1.00.00.0 \n",
            "0.01.01.0 0.00.00.0 0.01.00.0 1.01.00.0 1.00.00.0 0.00.0-1.0 -1.0-1.0-1.0 -1.0-1.0-1.0 0.0-1.0-1.0 0.00.00.0 1.00.00.0 \n",
            "0.00.00.0 0.00.00.0 0.01.00.0 0.01.00.0 0.01.00.0 0.00.00.0 0.0-1.0-1.0 -1.0-1.0-1.0 0.0-1.0-1.0 0.00.00.0 0.01.00.0 \n",
            "0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.01.00.0 1.00.00.0 0.0-1.00.0 0.0-1.0-1.0 0.0-1.0-1.0 0.00.00.0 0.01.01.0 \n",
            "0.00.00.0 0.00.00.0 0.01.00.0 0.00.00.0 0.00.00.0 0.01.00.0 1.00.00.0 -1.0-1.0-1.0 -1.0-1.0-1.0 0.00.01.0 1.00.01.0 \n",
            "0.00.01.0 0.01.00.0 0.01.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.00.00.0 0.0-1.0-1.0 0.0-1.0-1.0 0.00.00.0 0.00.00.0 \n",
            "-1.00.00.0 0.00.00.0 0.00.01.0 0.00.01.0 0.00.00.0 0.00.00.0 0.00.00.0 0.0-1.0-1.0 0.0-1.00.0 -1.0-1.00.0 0.00.01.0 \n",
            "Next Filter: \n",
            "2cwt/bias:0 weights : \n",
            "[ 0. -1.  0.  1.  0.  0.  0. -1.  0.  0. -1. -1. -1. -1. -1.]\n",
            "tea_2/bias:0 weights : \n",
            "[ 1. -1.  1.  0.  0.  0.  0. -1.  1. -1.  1.  0.  1.  0.  0.  0.  0.  0.\n",
            "  1. -1.  1. -1.  1.  0.  1. -1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.\n",
            "  0.  0.  0. -1.  1. -1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0. -1.  0. -1.  1. -1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.\n",
            "  1.  0.  0.  1.  0.  0.  0.  0.  1. -1.  0.  0.  0.  0. -1. -1.  1.  0.\n",
            "  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  0.  0. -1.\n",
            "  0.  0.  1.  0.  1.  0.  1. -1. -1. -1.  1. -1.]\n",
            "Post-Ternary Constraint Accuracy: \n",
            "Test Loss:  2.118959665298462\n",
            "Test Accuracy:  0.3319999873638153\n",
            "Original Floating-Point Accuracy: \n",
            "Test Loss:  1.5061702728271484\n",
            "Test Accuracy:  0.4700999855995178\n",
            "Accuracy loss due to train-then-constrain:  0.13809999823570251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0uHEo6WZFwT",
        "outputId": "84246c9a-1684-4b9d-8da0-4ed7e5c09287"
      },
      "source": [
        "inputsCWT = Input(shape=(32,32,3))\n",
        "\n",
        "# networkCWT = CWTConv2D(filters=10,\n",
        "# \t\t\t\t  kernel_size=(3,3),\n",
        "# \t\t\t\t  strides=(1,1)\n",
        "# \t\t\t\t  #activation='relu',\n",
        "# \t\t\t\t  #kernel_regularizer=regularizers.l1(l=0.1),\n",
        "# \t\t\t\t  #use_bias=True,\n",
        "# \t\t\t\t  )(inputsCWT)\n",
        "\n",
        "network2CWT = CWTConv2D(filters=15,\n",
        "\t\t\t\t  kernel_size=(11,11),\n",
        "\t\t\t\t  strides=(1,1)\n",
        "\t\t\t\t  #activation='relu',\n",
        "\t\t\t\t  #kernel_regularizer=regularizers.l1(l=0.1),\n",
        "\t\t\t\t  #use_bias=True,\n",
        "\t\t\t\t  )(inputsCWT)\n",
        "\n",
        "pooledcwt = CWTMaxPooling2D(pool_size=(2,2),strides=(2,2))(network2CWT)\n",
        "\n",
        "dropoutCWT = Dropout(0.4)(pooledcwt)\n",
        "\n",
        "flattenedCWT = Flatten()(dropoutCWT)\n",
        "\n",
        "tea = Tea(units=120, name=\"tea_22\")(flattenedCWT)\n",
        "\n",
        "poolingCWT = AdditivePooling(10)(tea)\n",
        "\n",
        "predictionsCWT = Activation('softmax')(poolingCWT)\n",
        "\n",
        "modelCWT = Model(inputs=inputsCWT, outputs=predictionsCWT)\n",
        "\n",
        "modelCWT.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.reshape(-1, 32,32,3) \n",
        "x_test = x_test.reshape(-1, 32,32,3)\n",
        "\n",
        "modelCWT.fit(x_train, y_train, batch_size=128, epochs=100, verbose=1, validation_split=0.2)\n",
        "\n",
        "scoreCWT = modelCWT.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Test Loss: \", scoreCWT[0]) \n",
        "print(\"Test Accuracy: \", scoreCWT[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - 4s 10ms/step - loss: 2.4810 - accuracy: 0.1320 - val_loss: 2.2008 - val_accuracy: 0.1595\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.2022 - accuracy: 0.1891 - val_loss: 2.1316 - val_accuracy: 0.1923\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.1403 - accuracy: 0.2253 - val_loss: 2.0811 - val_accuracy: 0.2364\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.1066 - accuracy: 0.2433 - val_loss: 2.0530 - val_accuracy: 0.2524\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 2.0648 - accuracy: 0.2622 - val_loss: 2.0433 - val_accuracy: 0.2543\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.0330 - accuracy: 0.2766 - val_loss: 2.0146 - val_accuracy: 0.2772\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 2.0235 - accuracy: 0.2877 - val_loss: 1.9962 - val_accuracy: 0.2735\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 2.0100 - accuracy: 0.2899 - val_loss: 1.9847 - val_accuracy: 0.2768\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.9936 - accuracy: 0.2943 - val_loss: 1.9693 - val_accuracy: 0.2762\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.9754 - accuracy: 0.2980 - val_loss: 1.9708 - val_accuracy: 0.2910\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.9647 - accuracy: 0.3080 - val_loss: 1.9467 - val_accuracy: 0.3043\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.9430 - accuracy: 0.3160 - val_loss: 1.9310 - val_accuracy: 0.2996\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.9285 - accuracy: 0.3230 - val_loss: 1.9107 - val_accuracy: 0.3104\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.9047 - accuracy: 0.3349 - val_loss: 1.9025 - val_accuracy: 0.3174\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.8824 - accuracy: 0.3433 - val_loss: 1.8850 - val_accuracy: 0.3189\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.8828 - accuracy: 0.3412 - val_loss: 1.8945 - val_accuracy: 0.3292\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.8740 - accuracy: 0.3473 - val_loss: 1.8716 - val_accuracy: 0.3205\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.8578 - accuracy: 0.3549 - val_loss: 1.8649 - val_accuracy: 0.3237\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.8413 - accuracy: 0.3620 - val_loss: 1.8556 - val_accuracy: 0.3279\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.8232 - accuracy: 0.3702 - val_loss: 1.8600 - val_accuracy: 0.3384\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.8277 - accuracy: 0.3672 - val_loss: 1.8499 - val_accuracy: 0.3418\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.8238 - accuracy: 0.3688 - val_loss: 1.8371 - val_accuracy: 0.3411\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7982 - accuracy: 0.3763 - val_loss: 1.8174 - val_accuracy: 0.3414\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7766 - accuracy: 0.3823 - val_loss: 1.8160 - val_accuracy: 0.3496\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7851 - accuracy: 0.3765 - val_loss: 1.8149 - val_accuracy: 0.3394\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7679 - accuracy: 0.3876 - val_loss: 1.7996 - val_accuracy: 0.3535\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7643 - accuracy: 0.3925 - val_loss: 1.7916 - val_accuracy: 0.3575\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7499 - accuracy: 0.3906 - val_loss: 1.7971 - val_accuracy: 0.3564\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7571 - accuracy: 0.3885 - val_loss: 1.7822 - val_accuracy: 0.3544\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7412 - accuracy: 0.3913 - val_loss: 1.7905 - val_accuracy: 0.3505\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7483 - accuracy: 0.3934 - val_loss: 1.7686 - val_accuracy: 0.3558\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7344 - accuracy: 0.3960 - val_loss: 1.7719 - val_accuracy: 0.3486\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7286 - accuracy: 0.4035 - val_loss: 1.7890 - val_accuracy: 0.3497\n",
            "Epoch 34/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7383 - accuracy: 0.3925 - val_loss: 1.7725 - val_accuracy: 0.3625\n",
            "Epoch 35/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7328 - accuracy: 0.4004 - val_loss: 1.7548 - val_accuracy: 0.3670\n",
            "Epoch 36/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7153 - accuracy: 0.4040 - val_loss: 1.7668 - val_accuracy: 0.3726\n",
            "Epoch 37/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7042 - accuracy: 0.4106 - val_loss: 1.7526 - val_accuracy: 0.3639\n",
            "Epoch 38/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6915 - accuracy: 0.4080 - val_loss: 1.7614 - val_accuracy: 0.3683\n",
            "Epoch 39/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6970 - accuracy: 0.4116 - val_loss: 1.7759 - val_accuracy: 0.3613\n",
            "Epoch 40/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7105 - accuracy: 0.4030 - val_loss: 1.7429 - val_accuracy: 0.3657\n",
            "Epoch 41/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6830 - accuracy: 0.4178 - val_loss: 1.7427 - val_accuracy: 0.3769\n",
            "Epoch 42/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6908 - accuracy: 0.4152 - val_loss: 1.7504 - val_accuracy: 0.3715\n",
            "Epoch 43/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6791 - accuracy: 0.4204 - val_loss: 1.7489 - val_accuracy: 0.3742\n",
            "Epoch 44/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6793 - accuracy: 0.4152 - val_loss: 1.7456 - val_accuracy: 0.3802\n",
            "Epoch 45/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6701 - accuracy: 0.4220 - val_loss: 1.7342 - val_accuracy: 0.3736\n",
            "Epoch 46/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6679 - accuracy: 0.4216 - val_loss: 1.7256 - val_accuracy: 0.3824\n",
            "Epoch 47/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6733 - accuracy: 0.4201 - val_loss: 1.7350 - val_accuracy: 0.3694\n",
            "Epoch 48/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6602 - accuracy: 0.4224 - val_loss: 1.7424 - val_accuracy: 0.3754\n",
            "Epoch 49/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6572 - accuracy: 0.4289 - val_loss: 1.7272 - val_accuracy: 0.3727\n",
            "Epoch 50/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6484 - accuracy: 0.4275 - val_loss: 1.7420 - val_accuracy: 0.3809\n",
            "Epoch 51/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6601 - accuracy: 0.4236 - val_loss: 1.7228 - val_accuracy: 0.3803\n",
            "Epoch 52/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6509 - accuracy: 0.4277 - val_loss: 1.7212 - val_accuracy: 0.3836\n",
            "Epoch 53/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6536 - accuracy: 0.4305 - val_loss: 1.7120 - val_accuracy: 0.3847\n",
            "Epoch 54/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6475 - accuracy: 0.4276 - val_loss: 1.7419 - val_accuracy: 0.3813\n",
            "Epoch 55/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6407 - accuracy: 0.4323 - val_loss: 1.7374 - val_accuracy: 0.3801\n",
            "Epoch 56/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6450 - accuracy: 0.4297 - val_loss: 1.7124 - val_accuracy: 0.3908\n",
            "Epoch 57/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6358 - accuracy: 0.4355 - val_loss: 1.7073 - val_accuracy: 0.3906\n",
            "Epoch 58/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6422 - accuracy: 0.4299 - val_loss: 1.7042 - val_accuracy: 0.3915\n",
            "Epoch 59/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6264 - accuracy: 0.4362 - val_loss: 1.7331 - val_accuracy: 0.3797\n",
            "Epoch 60/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6356 - accuracy: 0.4343 - val_loss: 1.6992 - val_accuracy: 0.3945\n",
            "Epoch 61/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6330 - accuracy: 0.4331 - val_loss: 1.7028 - val_accuracy: 0.3921\n",
            "Epoch 62/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6261 - accuracy: 0.4361 - val_loss: 1.6962 - val_accuracy: 0.3973\n",
            "Epoch 63/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6248 - accuracy: 0.4363 - val_loss: 1.6990 - val_accuracy: 0.3893\n",
            "Epoch 64/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6151 - accuracy: 0.4434 - val_loss: 1.7172 - val_accuracy: 0.3865\n",
            "Epoch 65/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6147 - accuracy: 0.4392 - val_loss: 1.7145 - val_accuracy: 0.3952\n",
            "Epoch 66/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6301 - accuracy: 0.4300 - val_loss: 1.6919 - val_accuracy: 0.4006\n",
            "Epoch 67/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6194 - accuracy: 0.4416 - val_loss: 1.6879 - val_accuracy: 0.3932\n",
            "Epoch 68/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6130 - accuracy: 0.4404 - val_loss: 1.7600 - val_accuracy: 0.3697\n",
            "Epoch 69/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6380 - accuracy: 0.4352 - val_loss: 1.6869 - val_accuracy: 0.3946\n",
            "Epoch 70/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6082 - accuracy: 0.4433 - val_loss: 1.6914 - val_accuracy: 0.3919\n",
            "Epoch 71/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6175 - accuracy: 0.4420 - val_loss: 1.7004 - val_accuracy: 0.3924\n",
            "Epoch 72/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6119 - accuracy: 0.4390 - val_loss: 1.6978 - val_accuracy: 0.4013\n",
            "Epoch 73/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6118 - accuracy: 0.4426 - val_loss: 1.6941 - val_accuracy: 0.3941\n",
            "Epoch 74/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6024 - accuracy: 0.4444 - val_loss: 1.6939 - val_accuracy: 0.3964\n",
            "Epoch 75/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.6046 - accuracy: 0.4466 - val_loss: 1.6884 - val_accuracy: 0.3971\n",
            "Epoch 76/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5968 - accuracy: 0.4486 - val_loss: 1.6946 - val_accuracy: 0.3997\n",
            "Epoch 77/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5937 - accuracy: 0.4468 - val_loss: 1.6918 - val_accuracy: 0.3860\n",
            "Epoch 78/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5901 - accuracy: 0.4496 - val_loss: 1.6696 - val_accuracy: 0.4041\n",
            "Epoch 79/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5938 - accuracy: 0.4538 - val_loss: 1.6857 - val_accuracy: 0.3975\n",
            "Epoch 80/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5893 - accuracy: 0.4492 - val_loss: 1.6832 - val_accuracy: 0.3997\n",
            "Epoch 81/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5972 - accuracy: 0.4515 - val_loss: 1.6788 - val_accuracy: 0.3963\n",
            "Epoch 82/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5822 - accuracy: 0.4520 - val_loss: 1.6830 - val_accuracy: 0.4004\n",
            "Epoch 83/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5797 - accuracy: 0.4534 - val_loss: 1.6872 - val_accuracy: 0.3984\n",
            "Epoch 84/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5903 - accuracy: 0.4517 - val_loss: 1.6891 - val_accuracy: 0.3943\n",
            "Epoch 85/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5714 - accuracy: 0.4590 - val_loss: 1.6976 - val_accuracy: 0.3970\n",
            "Epoch 86/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5824 - accuracy: 0.4539 - val_loss: 1.6786 - val_accuracy: 0.3977\n",
            "Epoch 87/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5731 - accuracy: 0.4522 - val_loss: 1.6940 - val_accuracy: 0.4003\n",
            "Epoch 88/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5758 - accuracy: 0.4527 - val_loss: 1.6789 - val_accuracy: 0.4001\n",
            "Epoch 89/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5804 - accuracy: 0.4551 - val_loss: 1.6852 - val_accuracy: 0.4004\n",
            "Epoch 90/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5580 - accuracy: 0.4616 - val_loss: 1.6764 - val_accuracy: 0.4016\n",
            "Epoch 91/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5739 - accuracy: 0.4574 - val_loss: 1.7074 - val_accuracy: 0.3837\n",
            "Epoch 92/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5691 - accuracy: 0.4578 - val_loss: 1.6711 - val_accuracy: 0.4050\n",
            "Epoch 93/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5799 - accuracy: 0.4507 - val_loss: 1.6821 - val_accuracy: 0.3993\n",
            "Epoch 94/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5768 - accuracy: 0.4569 - val_loss: 1.6639 - val_accuracy: 0.3980\n",
            "Epoch 95/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5543 - accuracy: 0.4674 - val_loss: 1.6744 - val_accuracy: 0.4016\n",
            "Epoch 96/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5642 - accuracy: 0.4647 - val_loss: 1.6795 - val_accuracy: 0.4006\n",
            "Epoch 97/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5679 - accuracy: 0.4574 - val_loss: 1.6898 - val_accuracy: 0.4005\n",
            "Epoch 98/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5513 - accuracy: 0.4642 - val_loss: 1.6572 - val_accuracy: 0.4067\n",
            "Epoch 99/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5583 - accuracy: 0.4644 - val_loss: 1.6672 - val_accuracy: 0.4063\n",
            "Epoch 100/100\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5582 - accuracy: 0.4628 - val_loss: 1.6597 - val_accuracy: 0.4014\n",
            "Test Loss:  1.6675361394882202\n",
            "Test Accuracy:  0.4041000008583069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jTwboT9bTuN",
        "outputId": "f144a055-9cd3-44df-a238-d50c53f2daa0"
      },
      "source": [
        "orig_weightsCWT, constrained_weightsCWT = constrain_weights_cwt(modelCWT)\n",
        "modelCWT.set_weights(constrained_weightsCWT)\n",
        "\t\t\n",
        "\n",
        "print(\"Post-Ternary Constraint Accuracy: \")\n",
        "\n",
        "# Evaluate the constained weight model\n",
        "scoreCWT = modelCWT.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "test_predictionsCWT = modelCWT.predict(x_test)\n",
        "\n",
        "print(\"Test Loss: \", scoreCWT[0])\n",
        "print(\"Test Accuracy: \", scoreCWT[1])\n",
        "\n",
        "# Restore the original weights temporarily so we can evaluate them\n",
        "modelCWT.set_weights(orig_weightsCWT)\n",
        "\n",
        "print(\"Original Floating-Point Accuracy: \")\n",
        "\n",
        "# Evaluate the original, floating-point weight model\n",
        "float_scoreCWT = modelCWT.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Test Loss: \", float_scoreCWT[0])\n",
        "print(\"Test Accuracy: \", float_scoreCWT[1])\n",
        "\n",
        "print(\"Accuracy loss due to train-then-constrain: \" ,float_scoreCWT[1] - scoreCWT[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Post-Ternary Constraint Accuracy: \n",
            "Test Loss:  1.6675361394882202\n",
            "Test Accuracy:  0.4041000008583069\n",
            "Original Floating-Point Accuracy: \n",
            "Test Loss:  1.6675361394882202\n",
            "Test Accuracy:  0.4041000008583069\n",
            "Accuracy loss due to train-then-constrain:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}